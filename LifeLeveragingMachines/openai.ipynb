{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99cc019b-fdab-4b4e-be87-8115395babb4",
   "metadata": {},
   "source": [
    "# Sandboxing \"Open\"Ai Stuff\n",
    "\n",
    "#### Some of the Models\n",
    "Handy reference.\n",
    "See the full dig here: https://platform.openai.com/docs/models\n",
    "\n",
    "##### GPT-3.5\n",
    "- `gpt-3.5-turbo`: 4096\n",
    "\n",
    "##### GPT-4\n",
    "- `gpt-4`: 8192\n",
    "- `gpt-4-32k`: 32768\n",
    "\n",
    "Don't know where they keep their pricing info........."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4ef779-dc56-4d3f-95d9-2082935f6df5",
   "metadata": {},
   "source": [
    "## `tiktoken`\n",
    "I want to count how many tokens some string would cost.\n",
    "\n",
    "Reference:\n",
    "https://github.com/openai/openai-cookbook/blob/c3cdf654d651b7d3614a54de8e60d294a54b3b21/examples/How_to_count_tokens_with_tiktoken.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b817de8b-104c-4800-b3ac-e1898372edee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7653ccf-d5c1-46ed-884c-1f0a706862ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "gpt35 = 'gpt-3.5-turbo'\n",
    "gpt4  = 'gpt-4'\n",
    "\n",
    "# Encodings\n",
    "enc35 = tiktoken.encoding_for_model(gpt35)\n",
    "enc4  = tiktoken.encoding_for_model(gpt4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64c9b03-af6b-492f-81b2-de3f0326f500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33883, 27439, 24578, 2503, 28311, 11, 36240, 59024, 31160, 627, 53, 344, 56455, 4244, 355, 10732, 333, 408, 2700, 83, 1960, 13, 1322, 258, 79424, 11, 72648, 520, 70100, 648, 198, 10274, 333, 408, 11, 87834, 16920, 355, 5275, 292, 5724, 5347, 11, 9231, 4582, 80757, 275, 90939, 66967, 64, 49580, 24578, 13, 720, 3360, 64, 49580, 93485, 71942, 13, 13182, 40027, 324, 259, 2910, 57143, 93485, 11163, 69252, 43391, 295, 345, 266, 70100, 648, 15952, 84, 18434, 14210, 13, 14925, 87801, 64220, 2322, 355, 61550, 13, 36378, 29059, 4849, 384, 7114, 300, 506, 345, 307, 2449, 372, 82943, 390, 38768, 264, 13, 720, 30635, 762, 28043, 11163, 61550, 304, 2322, 355, 7893, 9799, 277, 37232, 1238, 13, 17578, 55509, 285, 11, 44979, 355, 887, 5804, 96244, 198, 70546, 10952, 11, 90939, 99078, 285, 36240, 99078, 285, 11, 4499, 332, 4781, 259, 2910, 57143, 44979, 355, 198, 59367, 285, 2503, 28311, 16920, 355, 13, 56988, 581, 16903, 390, 38768, 9955, 3904, 372, 841, 593, 304, 3369, 19195, 324, 627, 3438, 300, 616, 355, 4582, 80757, 275, 477, 5979, 264, 24578, 43391, 309, 11, 70763, 58424, 9667, 18515, 285, 6587, 372, 627, 50, 291, 24905, 3916, 2781, 266, 9231, 24768, 25547, 384, 7114, 300, 13, 1322, 258, 1156, 8023, 96643, 70763, 19861, 361, 2536, 22408, 263, 57440, 13, 720, 44, 4202, 285, 6587, 372, 16831, 269, 8791, 99078, 285, 41253, 371, 285, 1172, 773, 10574, 13, 13182, 40027, 324, 25000, 86200, 79424, 198, 8318, 2503, 28311, 92034, 16903, 13, 42136, 56455, 9231, 25000, 86200, 29413, 5724, 13]\n",
      "256\n"
     ]
    }
   ],
   "source": [
    "example_text = \"\"\"Lorem ipsum dolor sit amet, consectetur adipiscing elit.\n",
    "Vivamus varius eleifend porttitor. Proin commodo, velit at molestie\n",
    "eleifend, odio lectus vehicula sem, vel suscipit quam urna quis dolor. \n",
    "Nulla quis lorem massa. Curabitur tincidunt lorem sed nisi aliquet,\n",
    "at molestie arcu viverra. Aliquam vitae metus nulla. Sed gravida egestas ex,\n",
    "id elementum justo congue a. \n",
    "Maecenas sed nulla in metus pulvinar ultrices. Ut mollis, lacus id convallis\n",
    "rutrum, quam mauris consectetur mauris, volutpat tincidunt lacus\n",
    "felis sit amet lectus. Vestibulum congue condimentum neque in efficitur.\n",
    "Phasellus suscipit orci a dolor aliquam, eget facilisis felis dictum.\n",
    "Sed sodales erat vel bibendum egestas. Proin scelerisque eget augue non rhoncus. \n",
    "Mauris dictum tortor ut mauris lobortis feugiat. Curabitur malesuada commodo\n",
    "mi sit amet vestibulum. Vivamus vel malesuada ligula.\"\"\"\n",
    "\n",
    "# Turn that text into tokens.\n",
    "example_encoding = enc35.encode(example_text)\n",
    "num_tokens = len(example_encoding)\n",
    "print(example_encoding)\n",
    "print(num_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25468f20-5f7c-444e-ad22-8fa87e0bde69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lorem ipsum dolor sit amet, consectetur adipiscing elit.\n",
      "Vivamus varius eleifend porttitor. Proin commodo, velit at molestie\n",
      "eleifend, odio lectus vehicula sem, vel suscipit quam urna quis dolor. \n",
      "Nulla quis lorem massa. Curabitur tincidunt lorem sed nisi aliquet,\n",
      "at molestie arcu viverra. Aliquam vitae metus nulla. Sed gravida egestas ex,\n",
      "id elementum justo congue a. \n",
      "Maecenas sed nulla in metus pulvinar ultrices. Ut mollis, lacus id convallis\n",
      "rutrum, quam mauris consectetur mauris, volutpat tincidunt lacus\n",
      "felis sit amet lectus. Vestibulum congue condimentum neque in efficitur.\n",
      "Phasellus suscipit orci a dolor aliquam, eget facilisis felis dictum.\n",
      "Sed sodales erat vel bibendum egestas. Proin scelerisque eget augue non rhoncus. \n",
      "Mauris dictum tortor ut mauris lobortis feugiat. Curabitur malesuada commodo\n",
      "mi sit amet vestibulum. Vivamus vel malesuada ligula.\n"
     ]
    }
   ],
   "source": [
    "example_decoding = enc35.decode(example_encoding)\n",
    "print(example_decoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efc7dc3-09b4-41b7-9b92-8ced367fda28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0301\"):\n",
    "    \"\"\"Returns the number of tokens used by a list of messages.\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        print(\"Warning: model not found. Using cl100k_base encoding.\")\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    if model == \"gpt-3.5-turbo\":\n",
    "        print(\"Warning: gpt-3.5-turbo may change over time. Returning num tokens assuming gpt-3.5-turbo-0301.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0301\")\n",
    "    elif model == \"gpt-4\":\n",
    "        print(\"Warning: gpt-4 may change over time. Returning num tokens assuming gpt-4-0314.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-4-0314\")\n",
    "    elif model == \"gpt-3.5-turbo-0301\":\n",
    "        tokens_per_message = 4  # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n",
    "        tokens_per_name = -1  # if there's a name, the role is omitted\n",
    "    elif model == \"gpt-4-0314\":\n",
    "        tokens_per_message = 3\n",
    "        tokens_per_name = 1\n",
    "    else:\n",
    "        raise NotImplementedError(f\"\"\"num_tokens_from_messages() is not implemented for model {model}. See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\")\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b880069-4266-4bdb-abe7-c180779d8493",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def num_tokens_from_messages(messages: list[dict[str, str]], model=\"gpt-3.5-turbo-0301\"):\n",
    "    \"\"\"Returns the number of tokens used by a list of messages.\"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "    except KeyError:\n",
    "        print(\"Warning: model not found. Using cl100k_base encoding.\")\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    if model == \"gpt-3.5-turbo\":\n",
    "        print(\"Warning: gpt-3.5-turbo may change over time. Returning num tokens assuming gpt-3.5-turbo-0301.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0301\")\n",
    "    elif model == \"gpt-4\":\n",
    "        print(\"Warning: gpt-4 may change over time. Returning num tokens assuming gpt-4-0314.\")\n",
    "        return num_tokens_from_messages(messages, model=\"gpt-4-0314\")\n",
    "    elif model == \"gpt-3.5-turbo-0301\":\n",
    "        tokens_per_message = 4  # every message follows <|start|>{role/name}\\n{content}<|end|>\\n\n",
    "        tokens_per_name = -1  # if there's a name, the role is omitted\n",
    "    elif model == \"gpt-4-0314\":\n",
    "        tokens_per_message = 3\n",
    "        tokens_per_name = 1\n",
    "    else:\n",
    "        raise NotImplementedError(f\"\"\"num_tokens_from_messages() is not implemented for model {model}. See https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\")\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3  # every reply is primed with <|start|>assistant<|message|>\n",
    "    return num_tokens\n",
    "\n",
    "\n",
    "example_messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpful, pattern-following assistant that translates corporate jargon into plain English.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"name\": \"example_user\",\n",
    "        \"content\": \"New synergies will help drive top-line growth.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"name\": \"example_assistant\",\n",
    "        \"content\": \"Things working well together will increase revenue.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"name\": \"example_user\",\n",
    "        \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"name\": \"example_assistant\",\n",
    "        \"content\": \"Let's talk later when we're less busy about how to do better.\",\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\",\n",
    "    },\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23775ac-c467-4748-8b76-d748c9a3523d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay, so I just want to know how many tokens for a single message or string.\n",
    "\n",
    "def get_num_tokens_of_message(message: str, model=gpt35):\n",
    "    encoder = tiktoken.encoding_for_model(model)\n",
    "    encoded_message = encoder.encode(message)\n",
    "    num_tokens = len(encoded_message)\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf016d87-805c-485d-a858-c4f37322d931",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_num_tokens_of_message(example_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff543c3a-5e7a-4287-80b4-664e621bb646",
   "metadata": {},
   "source": [
    "## Cool, now let's encode an entire repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04616f7-6ec4-44da-a2e0-72e483c0bee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('entire_repo_in_text.txt') as txtfile:\n",
    "    repo_txt = txtfile.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a393d904-7406-4b4b-a129-705a8caa26e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1975651"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(repo_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e72393-f334-4d9a-ab62-0317f9b3fd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1975651, that's a spicy meatball\n",
    "num_tokens_repo = get_num_tokens_of_message(repo_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19334d8-267d-4c22-b05d-d30107c0b571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "452257"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens_repo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
