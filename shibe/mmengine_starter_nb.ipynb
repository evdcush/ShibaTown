{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15 Minutes to Get Started with MMEngine\n",
    "\n",
    "https://mmengine.readthedocs.io/en/latest/get_started/15_minutes.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms.v2 as T\n",
    "import torchvision.transforms.v2.functional as TF\n",
    "\n",
    "from mmengine.model import BaseModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tutorial uses a ResNet50 from `torchvision.models`, but let's take a look at what our options are through `torchvision`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alexnet',\n",
      " 'convnext_base',\n",
      " 'convnext_large',\n",
      " 'convnext_small',\n",
      " 'convnext_tiny',\n",
      " 'deeplabv3_mobilenet_v3_large',\n",
      " 'deeplabv3_resnet101',\n",
      " 'deeplabv3_resnet50',\n",
      " 'densenet121',\n",
      " 'densenet161',\n",
      " 'densenet169',\n",
      " 'densenet201',\n",
      " 'efficientnet_b0',\n",
      " 'efficientnet_b1',\n",
      " 'efficientnet_b2',\n",
      " 'efficientnet_b3',\n",
      " 'efficientnet_b4',\n",
      " 'efficientnet_b5',\n",
      " 'efficientnet_b6',\n",
      " 'efficientnet_b7',\n",
      " 'efficientnet_v2_l',\n",
      " 'efficientnet_v2_m',\n",
      " 'efficientnet_v2_s',\n",
      " 'fasterrcnn_mobilenet_v3_large_320_fpn',\n",
      " 'fasterrcnn_mobilenet_v3_large_fpn',\n",
      " 'fasterrcnn_resnet50_fpn',\n",
      " 'fasterrcnn_resnet50_fpn_v2',\n",
      " 'fcn_resnet101',\n",
      " 'fcn_resnet50',\n",
      " 'fcos_resnet50_fpn',\n",
      " 'googlenet',\n",
      " 'inception_v3',\n",
      " 'keypointrcnn_resnet50_fpn',\n",
      " 'lraspp_mobilenet_v3_large',\n",
      " 'maskrcnn_resnet50_fpn',\n",
      " 'maskrcnn_resnet50_fpn_v2',\n",
      " 'maxvit_t',\n",
      " 'mc3_18',\n",
      " 'mnasnet0_5',\n",
      " 'mnasnet0_75',\n",
      " 'mnasnet1_0',\n",
      " 'mnasnet1_3',\n",
      " 'mobilenet_v2',\n",
      " 'mobilenet_v3_large',\n",
      " 'mobilenet_v3_small',\n",
      " 'mvit_v1_b',\n",
      " 'mvit_v2_s',\n",
      " 'quantized_googlenet',\n",
      " 'quantized_inception_v3',\n",
      " 'quantized_mobilenet_v2',\n",
      " 'quantized_mobilenet_v3_large',\n",
      " 'quantized_resnet18',\n",
      " 'quantized_resnet50',\n",
      " 'quantized_resnext101_32x8d',\n",
      " 'quantized_resnext101_64x4d',\n",
      " 'quantized_shufflenet_v2_x0_5',\n",
      " 'quantized_shufflenet_v2_x1_0',\n",
      " 'quantized_shufflenet_v2_x1_5',\n",
      " 'quantized_shufflenet_v2_x2_0',\n",
      " 'r2plus1d_18',\n",
      " 'r3d_18',\n",
      " 'raft_large',\n",
      " 'raft_small',\n",
      " 'regnet_x_16gf',\n",
      " 'regnet_x_1_6gf',\n",
      " 'regnet_x_32gf',\n",
      " 'regnet_x_3_2gf',\n",
      " 'regnet_x_400mf',\n",
      " 'regnet_x_800mf',\n",
      " 'regnet_x_8gf',\n",
      " 'regnet_y_128gf',\n",
      " 'regnet_y_16gf',\n",
      " 'regnet_y_1_6gf',\n",
      " 'regnet_y_32gf',\n",
      " 'regnet_y_3_2gf',\n",
      " 'regnet_y_400mf',\n",
      " 'regnet_y_800mf',\n",
      " 'regnet_y_8gf',\n",
      " 'resnet101',\n",
      " 'resnet152',\n",
      " 'resnet18',\n",
      " 'resnet34',\n",
      " 'resnet50',\n",
      " 'resnext101_32x8d',\n",
      " 'resnext101_64x4d',\n",
      " 'resnext50_32x4d',\n",
      " 'retinanet_resnet50_fpn',\n",
      " 'retinanet_resnet50_fpn_v2',\n",
      " 's3d',\n",
      " 'shufflenet_v2_x0_5',\n",
      " 'shufflenet_v2_x1_0',\n",
      " 'shufflenet_v2_x1_5',\n",
      " 'shufflenet_v2_x2_0',\n",
      " 'squeezenet1_0',\n",
      " 'squeezenet1_1',\n",
      " 'ssd300_vgg16',\n",
      " 'ssdlite320_mobilenet_v3_large',\n",
      " 'swin3d_b',\n",
      " 'swin3d_s',\n",
      " 'swin3d_t',\n",
      " 'swin_b',\n",
      " 'swin_s',\n",
      " 'swin_t',\n",
      " 'swin_v2_b',\n",
      " 'swin_v2_s',\n",
      " 'swin_v2_t',\n",
      " 'vgg11',\n",
      " 'vgg11_bn',\n",
      " 'vgg13',\n",
      " 'vgg13_bn',\n",
      " 'vgg16',\n",
      " 'vgg16_bn',\n",
      " 'vgg19',\n",
      " 'vgg19_bn',\n",
      " 'vit_b_16',\n",
      " 'vit_b_32',\n",
      " 'vit_h_14',\n",
      " 'vit_l_16',\n",
      " 'vit_l_32',\n",
      " 'wide_resnet101_2',\n",
      " 'wide_resnet50_2']\n"
     ]
    }
   ],
   "source": [
    "# Print the list of available models.\n",
    "available_models = torchvision.models.list_models()\n",
    "pprint(available_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MMEngine provides some useful model analysis tools.\n",
    "# Let's analyse the number of parameters and flops of the model.\n",
    "from mmengine.analysis import get_model_complexity_info\n",
    "\n",
    "# This function returns an analysis of the model as a dict:\n",
    "#   out_table (str): a summary analysis of the model as a table.\n",
    "#       NB: this includes a the total params, flops, and activations.\n",
    "#   out_arch (str): an analysis inline with the repr of the model arch.\n",
    "#   activations_str (str): the total number of activations of the model.\n",
    "#   flops_str (str): the total flops of the model.\n",
    "#   params_str (str): the total number of params of the model.\n",
    "\n",
    "def summarize_model(model, input_shape=(3, 224, 224)):\n",
    "    analysis = get_model_complexity_info(model, input_shape)\n",
    "    table  = analysis['out_table']\n",
    "    print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_b_16 = torchvision.models.vit_b_16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02/04 16:59:52 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Unsupported operator aten::mul encountered 49 time(s)\n",
      "02/04 16:59:52 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Unsupported operator aten::add encountered 25 time(s)\n",
      "02/04 16:59:52 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Unsupported operator aten::div encountered 12 time(s)\n",
      "02/04 16:59:52 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Unsupported operator aten::unflatten encountered 12 time(s)\n",
      "02/04 16:59:52 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "02/04 16:59:52 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Unsupported operator aten::gelu encountered 12 time(s)\n",
      "02/04 16:59:52 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "encoder.layers.encoder_layer_0.self_attention.out_proj, encoder.layers.encoder_layer_1.self_attention.out_proj, encoder.layers.encoder_layer_10.self_attention.out_proj, encoder.layers.encoder_layer_11.self_attention.out_proj, encoder.layers.encoder_layer_2.self_attention.out_proj, encoder.layers.encoder_layer_3.self_attention.out_proj, encoder.layers.encoder_layer_4.self_attention.out_proj, encoder.layers.encoder_layer_5.self_attention.out_proj, encoder.layers.encoder_layer_6.self_attention.out_proj, encoder.layers.encoder_layer_7.self_attention.out_proj, encoder.layers.encoder_layer_8.self_attention.out_proj, encoder.layers.encoder_layer_9.self_attention.out_proj\n",
      "02/04 16:59:52 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Unsupported operator aten::layer_norm encountered 25 time(s)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "+------------------------------------+----------------------+-----------+--------------+\n",
      "|\u001b[1m \u001b[0m\u001b[1mmodule                            \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m#parameters or shape\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m#flops   \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m#activations\u001b[0m\u001b[1m \u001b[0m|\n",
      "+------------------------------------+----------------------+-----------+--------------+\n",
      "| model                              | 86.568M              | 16.867G   | 16.491M      |\n",
      "|  class_token                       |  (1, 1, 768)         |           |              |\n",
      "|  conv_proj                         |  0.591M              |  0.116G   |  0.151M      |\n",
      "|   conv_proj.weight                 |   (768, 3, 16, 16)   |           |              |\n",
      "|   conv_proj.bias                   |   (768,)             |           |              |\n",
      "|  encoder                           |  85.207M             |  16.751G  |  16.34M      |\n",
      "|   encoder.pos_embedding            |   (1, 197, 768)      |           |              |\n",
      "|   encoder.layers                   |   85.054M            |   16.75G  |   16.34M     |\n",
      "|    encoder.layers.encoder_layer_0  |    7.088M            |    1.396G |    1.362M    |\n",
      "|    encoder.layers.encoder_layer_1  |    7.088M            |    1.396G |    1.362M    |\n",
      "|    encoder.layers.encoder_layer_2  |    7.088M            |    1.396G |    1.362M    |\n",
      "|    encoder.layers.encoder_layer_3  |    7.088M            |    1.396G |    1.362M    |\n",
      "|    encoder.layers.encoder_layer_4  |    7.088M            |    1.396G |    1.362M    |\n",
      "|    encoder.layers.encoder_layer_5  |    7.088M            |    1.396G |    1.362M    |\n",
      "|    encoder.layers.encoder_layer_6  |    7.088M            |    1.396G |    1.362M    |\n",
      "|    encoder.layers.encoder_layer_7  |    7.088M            |    1.396G |    1.362M    |\n",
      "|    encoder.layers.encoder_layer_8  |    7.088M            |    1.396G |    1.362M    |\n",
      "|    encoder.layers.encoder_layer_9  |    7.088M            |    1.396G |    1.362M    |\n",
      "|    encoder.layers.encoder_layer_10 |    7.088M            |    1.396G |    1.362M    |\n",
      "|    encoder.layers.encoder_layer_11 |    7.088M            |    1.396G |    1.362M    |\n",
      "|   encoder.ln                       |   1.536K             |   0.756M  |   0          |\n",
      "|    encoder.ln.weight               |    (768,)            |           |              |\n",
      "|    encoder.ln.bias                 |    (768,)            |           |              |\n",
      "|  heads.head                        |  0.769M              |  0.768M   |  1K          |\n",
      "|   heads.head.weight                |   (1000, 768)        |           |              |\n",
      "|   heads.head.bias                  |   (1000,)            |           |              |\n",
      "+------------------------------------+----------------------+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summarize_model(vit_b_16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, ViT-base is pretty huge!\n",
    "Let's try Swin, see if there is any difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " swin_v2_t = torchvision.models. swin_v2_t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02/04 17:00:04 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Unsupported operator aten::sigmoid encountered 12 time(s)\n",
      "02/04 17:00:04 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Unsupported operator aten::mul encountered 82 time(s)\n",
      "02/04 17:00:04 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Unsupported operator aten::rsub encountered 24 time(s)\n",
      "02/04 17:00:04 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Unsupported operator aten::pad encountered 15 time(s)\n",
      "02/04 17:00:04 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Unsupported operator aten::clone encountered 12 time(s)\n",
      "02/04 17:00:04 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Unsupported operator aten::numel encountered 12 time(s)\n",
      "02/04 17:00:04 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Unsupported operator aten::zero_ encountered 12 time(s)\n",
      "02/04 17:00:04 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Unsupported operator aten::linalg_vector_norm encountered 24 time(s)\n",
      "02/04 17:00:04 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Unsupported operator aten::clamp_min encountered 24 time(s)\n",
      "02/04 17:00:04 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Unsupported operator aten::expand_as encountered 24 time(s)\n",
      "02/04 17:00:04 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Unsupported operator aten::div encountered 24 time(s)\n",
      "02/04 17:00:04 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Unsupported operator aten::exp encountered 12 time(s)\n",
      "02/04 17:00:04 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Unsupported operator aten::add encountered 41 time(s)\n",
      "02/04 17:00:04 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Unsupported operator aten::softmax encountered 12 time(s)\n",
      "02/04 17:00:04 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Unsupported operator aten::fill_ encountered 45 time(s)\n",
      "02/04 17:00:04 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Unsupported operator aten::sub encountered 5 time(s)\n",
      "02/04 17:00:04 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Unsupported operator aten::ne encountered 5 time(s)\n",
      "02/04 17:00:04 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Unsupported operator aten::bernoulli_ encountered 22 time(s)\n",
      "02/04 17:00:04 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Unsupported operator aten::div_ encountered 22 time(s)\n",
      "02/04 17:00:04 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "features.1.0.attn.proj, features.1.0.attn.qkv, features.1.0.stochastic_depth, features.1.1.attn.proj, features.1.1.attn.qkv, features.3.0.attn.proj, features.3.0.attn.qkv, features.3.1.attn.proj, features.3.1.attn.qkv, features.5.0.attn.proj, features.5.0.attn.qkv, features.5.1.attn.proj, features.5.1.attn.qkv, features.5.2.attn.proj, features.5.2.attn.qkv, features.5.3.attn.proj, features.5.3.attn.qkv, features.5.4.attn.proj, features.5.4.attn.qkv, features.5.5.attn.proj, features.5.5.attn.qkv, features.7.0.attn.proj, features.7.0.attn.qkv, features.7.1.attn.proj, features.7.1.attn.qkv\n",
      "02/04 17:00:04 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Unsupported operator aten::layer_norm encountered 29 time(s)\n",
      "02/04 17:00:04 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Unsupported operator aten::adaptive_avg_pool2d encountered 1 time(s)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "+-------------------------+----------------------+------------+--------------+\n",
      "|\u001b[1m \u001b[0m\u001b[1mmodule                 \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m#parameters or shape\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m#flops    \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m#activations\u001b[0m\u001b[1m \u001b[0m|\n",
      "+-------------------------+----------------------+------------+--------------+\n",
      "| model                   | 28.352M              | 4.956G     | 20.913M      |\n",
      "|  features               |  27.581M             |  4.955G    |  20.912M     |\n",
      "|   features.0            |   4.896K             |   15.956M  |   0.301M     |\n",
      "|    features.0.0         |    4.704K            |    14.451M |    0.301M    |\n",
      "|    features.0.2         |    0.192K            |    1.505M  |    0         |\n",
      "|   features.1            |   0.23M              |   0.778G   |   7.457M     |\n",
      "|    features.1.0         |    0.115M            |    0.389G  |    3.729M    |\n",
      "|    features.1.1         |    0.115M            |    0.389G  |    3.729M    |\n",
      "|   features.2            |   74.112K            |   58.555M  |   0.151M     |\n",
      "|    features.2.reduction |    73.728K           |    57.803M |    0.151M    |\n",
      "|    features.2.norm      |    0.384K            |    0.753M  |    0         |\n",
      "|   features.3            |   0.899M             |   0.82G    |   4.491M     |\n",
      "|    features.3.0         |    0.449M            |    0.41G   |    2.245M    |\n",
      "|    features.3.1         |    0.449M            |    0.41G   |    2.245M    |\n",
      "|   features.4            |   0.296M             |   58.179M  |   75.264K    |\n",
      "|    features.4.reduction |    0.295M            |    57.803M |    75.264K   |\n",
      "|    features.4.norm      |    0.768K            |    0.376M  |    0         |\n",
      "|   features.5            |   10.693M            |   2.383G   |   7.094M     |\n",
      "|    features.5.0         |    1.782M            |    0.397G  |    1.182M    |\n",
      "|    features.5.1         |    1.782M            |    0.397G  |    1.182M    |\n",
      "|    features.5.2         |    1.782M            |    0.397G  |    1.182M    |\n",
      "|    features.5.3         |    1.782M            |    0.397G  |    1.182M    |\n",
      "|    features.5.4         |    1.782M            |    0.397G  |    1.182M    |\n",
      "|    features.5.5         |    1.782M            |    0.397G  |    1.182M    |\n",
      "|   features.6            |   1.181M             |   57.991M  |   37.632K    |\n",
      "|    features.6.reduction |    1.18M             |    57.803M |    37.632K   |\n",
      "|    features.6.norm      |    1.536K            |    0.188M  |    0         |\n",
      "|   features.7            |   14.203M            |   0.784G   |   1.306M     |\n",
      "|    features.7.0         |    7.102M            |    0.392G  |    0.653M    |\n",
      "|    features.7.1         |    7.102M            |    0.392G  |    0.653M    |\n",
      "|  norm                   |  1.536K              |  0.188M    |  0           |\n",
      "|   norm.weight           |   (768,)             |            |              |\n",
      "|   norm.bias             |   (768,)             |            |              |\n",
      "|  head                   |  0.769M              |  0.768M    |  1K          |\n",
      "|   head.weight           |   (1000, 768)        |            |              |\n",
      "|   head.bias             |   (1000,)            |            |              |\n",
      "|  avgpool                |                      |  37.632K   |  0           |\n",
      "+-------------------------+----------------------+------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summarize_model(swin_v2_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's build a Model!\n",
    "\n",
    "We'll wrap a model provided from from `torchvision`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try the smallest ViT we can get, a ViT-B16.\n",
    "vit_b_16 = torchvision.models.vit_b_16()\n",
    "\n",
    "class MMViT_B16(BaseModel):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, imgs, labels, mode):\n",
    "        x = self.model(imgs)\n",
    "        if mode == 'loss':\n",
    "            return {'loss': F.cross_entropy(x, labels)}\n",
    "        elif mode == 'predict':\n",
    "            return x, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's get some data!\n",
    "\n",
    "We need to create a `Dataset` and `DataLoader` for training and validation.\n",
    "Let's grab a dataset from `torchvision`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/evan/.pyenv/versions/3.11.5/envs/3115/lib/python3.11/site-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# We'll be going for CIFAR10.\n",
    "\n",
    "dpath = '/home/evan/Data/cifar10'\n",
    "\n",
    "# First, let's define the transforms.\n",
    "norm_cfg = dict(\n",
    "    mean=[0.491, 0.482, 0.447],\n",
    "    std=[0.202, 0.199, 0.201]\n",
    ")\n",
    "\n",
    "# ---------------------------------  train  --------------------------------- #\n",
    "\n",
    "train_transforms = T.Compose([\n",
    "    T.RandomCrop(32, padding=4),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(**norm_cfg),\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    dpath,\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=train_transforms,\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(batch_size=32, shuffle=True, dataset=train_dataset)\n",
    "\n",
    "# ----------------------------------  val  ---------------------------------- #\n",
    "\n",
    "val_transforms = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(**norm_cfg),\n",
    "])\n",
    "\n",
    "val_dataset = torchvision.datasets.CIFAR10(\n",
    "    dpath,\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=val_transforms,\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(batch_size=32, shuffle=False, dataset=val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build an Evaluation Metric\n",
    "To evaluate model performance, we need to define a **Metric** called accuracy. \n",
    "This metric needs inherit from `BaseMetric` and implement the \n",
    "`process` and `compute_metrics` methods.\n",
    "\n",
    "`process`: \n",
    "This method receives a batch of data from the dataloader and outputs from the\n",
    "model (in `mode=\"predict\"`). \n",
    "After processing this data, we save the information to `self.results` property.\n",
    "\n",
    "```{tip}\n",
    "In a distributed environment, `self.results` is the information collected \n",
    "from all the processes.\n",
    "```\n",
    "\n",
    "`compute_metrics`:\n",
    "This method receives the `results` processed by `process` to calculate an\n",
    "evaluation metric, returning the information as a `dict`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3115",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
