{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15 Minutes to Get Started with MMEngine\n",
    "\n",
    "https://mmengine.readthedocs.io/en/latest/get_started/15_minutes.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/evan/.pyenv/versions/3.11.7/envs/3117/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms.v2 as T\n",
    "import torchvision.transforms.v2.functional as TF\n",
    "\n",
    "from mmengine.model import BaseModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tutorial uses a ResNet50 from `torchvision.models`, but let's take a look at what our options are through `torchvision`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alexnet',\n",
      " 'convnext_base',\n",
      " 'convnext_large',\n",
      " 'convnext_small',\n",
      " 'convnext_tiny',\n",
      " 'deeplabv3_mobilenet_v3_large',\n",
      " 'deeplabv3_resnet101',\n",
      " 'deeplabv3_resnet50',\n",
      " 'densenet121',\n",
      " 'densenet161',\n",
      " 'densenet169',\n",
      " 'densenet201',\n",
      " 'efficientnet_b0',\n",
      " 'efficientnet_b1',\n",
      " 'efficientnet_b2',\n",
      " 'efficientnet_b3',\n",
      " 'efficientnet_b4',\n",
      " 'efficientnet_b5',\n",
      " 'efficientnet_b6',\n",
      " 'efficientnet_b7',\n",
      " 'efficientnet_v2_l',\n",
      " 'efficientnet_v2_m',\n",
      " 'efficientnet_v2_s',\n",
      " 'fasterrcnn_mobilenet_v3_large_320_fpn',\n",
      " 'fasterrcnn_mobilenet_v3_large_fpn',\n",
      " 'fasterrcnn_resnet50_fpn',\n",
      " 'fasterrcnn_resnet50_fpn_v2',\n",
      " 'fcn_resnet101',\n",
      " 'fcn_resnet50',\n",
      " 'fcos_resnet50_fpn',\n",
      " 'googlenet',\n",
      " 'inception_v3',\n",
      " 'keypointrcnn_resnet50_fpn',\n",
      " 'lraspp_mobilenet_v3_large',\n",
      " 'maskrcnn_resnet50_fpn',\n",
      " 'maskrcnn_resnet50_fpn_v2',\n",
      " 'maxvit_t',\n",
      " 'mc3_18',\n",
      " 'mnasnet0_5',\n",
      " 'mnasnet0_75',\n",
      " 'mnasnet1_0',\n",
      " 'mnasnet1_3',\n",
      " 'mobilenet_v2',\n",
      " 'mobilenet_v3_large',\n",
      " 'mobilenet_v3_small',\n",
      " 'mvit_v1_b',\n",
      " 'mvit_v2_s',\n",
      " 'quantized_googlenet',\n",
      " 'quantized_inception_v3',\n",
      " 'quantized_mobilenet_v2',\n",
      " 'quantized_mobilenet_v3_large',\n",
      " 'quantized_resnet18',\n",
      " 'quantized_resnet50',\n",
      " 'quantized_resnext101_32x8d',\n",
      " 'quantized_resnext101_64x4d',\n",
      " 'quantized_shufflenet_v2_x0_5',\n",
      " 'quantized_shufflenet_v2_x1_0',\n",
      " 'quantized_shufflenet_v2_x1_5',\n",
      " 'quantized_shufflenet_v2_x2_0',\n",
      " 'r2plus1d_18',\n",
      " 'r3d_18',\n",
      " 'raft_large',\n",
      " 'raft_small',\n",
      " 'regnet_x_16gf',\n",
      " 'regnet_x_1_6gf',\n",
      " 'regnet_x_32gf',\n",
      " 'regnet_x_3_2gf',\n",
      " 'regnet_x_400mf',\n",
      " 'regnet_x_800mf',\n",
      " 'regnet_x_8gf',\n",
      " 'regnet_y_128gf',\n",
      " 'regnet_y_16gf',\n",
      " 'regnet_y_1_6gf',\n",
      " 'regnet_y_32gf',\n",
      " 'regnet_y_3_2gf',\n",
      " 'regnet_y_400mf',\n",
      " 'regnet_y_800mf',\n",
      " 'regnet_y_8gf',\n",
      " 'resnet101',\n",
      " 'resnet152',\n",
      " 'resnet18',\n",
      " 'resnet34',\n",
      " 'resnet50',\n",
      " 'resnext101_32x8d',\n",
      " 'resnext101_64x4d',\n",
      " 'resnext50_32x4d',\n",
      " 'retinanet_resnet50_fpn',\n",
      " 'retinanet_resnet50_fpn_v2',\n",
      " 's3d',\n",
      " 'shufflenet_v2_x0_5',\n",
      " 'shufflenet_v2_x1_0',\n",
      " 'shufflenet_v2_x1_5',\n",
      " 'shufflenet_v2_x2_0',\n",
      " 'squeezenet1_0',\n",
      " 'squeezenet1_1',\n",
      " 'ssd300_vgg16',\n",
      " 'ssdlite320_mobilenet_v3_large',\n",
      " 'swin3d_b',\n",
      " 'swin3d_s',\n",
      " 'swin3d_t',\n",
      " 'swin_b',\n",
      " 'swin_s',\n",
      " 'swin_t',\n",
      " 'swin_v2_b',\n",
      " 'swin_v2_s',\n",
      " 'swin_v2_t',\n",
      " 'vgg11',\n",
      " 'vgg11_bn',\n",
      " 'vgg13',\n",
      " 'vgg13_bn',\n",
      " 'vgg16',\n",
      " 'vgg16_bn',\n",
      " 'vgg19',\n",
      " 'vgg19_bn',\n",
      " 'vit_b_16',\n",
      " 'vit_b_32',\n",
      " 'vit_h_14',\n",
      " 'vit_l_16',\n",
      " 'vit_l_32',\n",
      " 'wide_resnet101_2',\n",
      " 'wide_resnet50_2']\n"
     ]
    }
   ],
   "source": [
    "# Print the list of available models.\n",
    "available_models = torchvision.models.list_models()\n",
    "pprint(available_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MMEngine provides some useful model analysis tools.\n",
    "# Let's analyse the number of parameters and flops of the model.\n",
    "from mmengine.analysis import get_model_complexity_info\n",
    "\n",
    "# This function returns an analysis of the model as a dict:\n",
    "#   out_table (str): a summary analysis of the model as a table.\n",
    "#       NB: this includes a the total params, flops, and activations.\n",
    "#   out_arch (str): an analysis inline with the repr of the model arch.\n",
    "#   activations_str (str): the total number of activations of the model.\n",
    "#   flops_str (str): the total flops of the model.\n",
    "#   params_str (str): the total number of params of the model.\n",
    "\n",
    "def summarize_model(model, input_shape=(3, 224, 224)):\n",
    "    analysis = get_model_complexity_info(model, input_shape)\n",
    "    table  = analysis['out_table']\n",
    "    print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_b_16 = torchvision.models.vit_b_16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02/04 17:37:24 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Unsupported operator aten::mul encountered 49 time(s)\n",
      "02/04 17:37:24 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Unsupported operator aten::add encountered 25 time(s)\n",
      "02/04 17:37:24 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Unsupported operator aten::div encountered 12 time(s)\n",
      "02/04 17:37:24 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Unsupported operator aten::unflatten encountered 12 time(s)\n",
      "02/04 17:37:24 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Unsupported operator aten::scaled_dot_product_attention encountered 12 time(s)\n",
      "02/04 17:37:24 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Unsupported operator aten::gelu encountered 12 time(s)\n",
      "02/04 17:37:24 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "encoder.layers.encoder_layer_0.self_attention.out_proj, encoder.layers.encoder_layer_1.self_attention.out_proj, encoder.layers.encoder_layer_10.self_attention.out_proj, encoder.layers.encoder_layer_11.self_attention.out_proj, encoder.layers.encoder_layer_2.self_attention.out_proj, encoder.layers.encoder_layer_3.self_attention.out_proj, encoder.layers.encoder_layer_4.self_attention.out_proj, encoder.layers.encoder_layer_5.self_attention.out_proj, encoder.layers.encoder_layer_6.self_attention.out_proj, encoder.layers.encoder_layer_7.self_attention.out_proj, encoder.layers.encoder_layer_8.self_attention.out_proj, encoder.layers.encoder_layer_9.self_attention.out_proj\n",
      "02/04 17:37:25 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Unsupported operator aten::layer_norm encountered 25 time(s)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "+------------------------------------+----------------------+-----------+--------------+\n",
      "|\u001b[1m \u001b[0m\u001b[1mmodule                            \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m#parameters or shape\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m#flops   \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m#activations\u001b[0m\u001b[1m \u001b[0m|\n",
      "+------------------------------------+----------------------+-----------+--------------+\n",
      "| model                              | 86.568M              | 16.867G   | 16.491M      |\n",
      "|  class_token                       |  (1, 1, 768)         |           |              |\n",
      "|  conv_proj                         |  0.591M              |  0.116G   |  0.151M      |\n",
      "|   conv_proj.weight                 |   (768, 3, 16, 16)   |           |              |\n",
      "|   conv_proj.bias                   |   (768,)             |           |              |\n",
      "|  encoder                           |  85.207M             |  16.751G  |  16.34M      |\n",
      "|   encoder.pos_embedding            |   (1, 197, 768)      |           |              |\n",
      "|   encoder.layers                   |   85.054M            |   16.75G  |   16.34M     |\n",
      "|    encoder.layers.encoder_layer_0  |    7.088M            |    1.396G |    1.362M    |\n",
      "|    encoder.layers.encoder_layer_1  |    7.088M            |    1.396G |    1.362M    |\n",
      "|    encoder.layers.encoder_layer_2  |    7.088M            |    1.396G |    1.362M    |\n",
      "|    encoder.layers.encoder_layer_3  |    7.088M            |    1.396G |    1.362M    |\n",
      "|    encoder.layers.encoder_layer_4  |    7.088M            |    1.396G |    1.362M    |\n",
      "|    encoder.layers.encoder_layer_5  |    7.088M            |    1.396G |    1.362M    |\n",
      "|    encoder.layers.encoder_layer_6  |    7.088M            |    1.396G |    1.362M    |\n",
      "|    encoder.layers.encoder_layer_7  |    7.088M            |    1.396G |    1.362M    |\n",
      "|    encoder.layers.encoder_layer_8  |    7.088M            |    1.396G |    1.362M    |\n",
      "|    encoder.layers.encoder_layer_9  |    7.088M            |    1.396G |    1.362M    |\n",
      "|    encoder.layers.encoder_layer_10 |    7.088M            |    1.396G |    1.362M    |\n",
      "|    encoder.layers.encoder_layer_11 |    7.088M            |    1.396G |    1.362M    |\n",
      "|   encoder.ln                       |   1.536K             |   0.756M  |   0          |\n",
      "|    encoder.ln.weight               |    (768,)            |           |              |\n",
      "|    encoder.ln.bias                 |    (768,)            |           |              |\n",
      "|  heads.head                        |  0.769M              |  0.768M   |  1K          |\n",
      "|   heads.head.weight                |   (1000, 768)        |           |              |\n",
      "|   heads.head.bias                  |   (1000,)            |           |              |\n",
      "+------------------------------------+----------------------+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summarize_model(vit_b_16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, ViT-base is pretty huge!\n",
    "Let's try Swin, see if there is any difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " swin_v2_t = torchvision.models. swin_v2_t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02/04 17:37:28 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Unsupported operator aten::sigmoid encountered 12 time(s)\n",
      "02/04 17:37:28 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Unsupported operator aten::mul encountered 82 time(s)\n",
      "02/04 17:37:28 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Unsupported operator aten::rsub encountered 24 time(s)\n",
      "02/04 17:37:28 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Unsupported operator aten::pad encountered 15 time(s)\n",
      "02/04 17:37:28 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Unsupported operator aten::clone encountered 12 time(s)\n",
      "02/04 17:37:28 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Unsupported operator aten::numel encountered 12 time(s)\n",
      "02/04 17:37:28 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Unsupported operator aten::zero_ encountered 12 time(s)\n",
      "02/04 17:37:28 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Unsupported operator aten::linalg_vector_norm encountered 24 time(s)\n",
      "02/04 17:37:28 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Unsupported operator aten::clamp_min encountered 24 time(s)\n",
      "02/04 17:37:28 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Unsupported operator aten::expand_as encountered 24 time(s)\n",
      "02/04 17:37:28 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Unsupported operator aten::div encountered 24 time(s)\n",
      "02/04 17:37:28 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Unsupported operator aten::exp encountered 12 time(s)\n",
      "02/04 17:37:28 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Unsupported operator aten::add encountered 41 time(s)\n",
      "02/04 17:37:28 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Unsupported operator aten::softmax encountered 12 time(s)\n",
      "02/04 17:37:28 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Unsupported operator aten::fill_ encountered 45 time(s)\n",
      "02/04 17:37:28 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Unsupported operator aten::sub encountered 5 time(s)\n",
      "02/04 17:37:28 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Unsupported operator aten::ne encountered 5 time(s)\n",
      "02/04 17:37:28 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Unsupported operator aten::bernoulli_ encountered 22 time(s)\n",
      "02/04 17:37:28 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Unsupported operator aten::div_ encountered 22 time(s)\n",
      "02/04 17:37:28 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - The following submodules of the model were never called during the trace of the graph. They may be unused, or they were accessed by direct calls to .forward() or via other python methods. In the latter case they will have zeros for statistics, though their statistics will still contribute to their parent calling module.\n",
      "features.1.0.attn.proj, features.1.0.attn.qkv, features.1.0.stochastic_depth, features.1.1.attn.proj, features.1.1.attn.qkv, features.3.0.attn.proj, features.3.0.attn.qkv, features.3.1.attn.proj, features.3.1.attn.qkv, features.5.0.attn.proj, features.5.0.attn.qkv, features.5.1.attn.proj, features.5.1.attn.qkv, features.5.2.attn.proj, features.5.2.attn.qkv, features.5.3.attn.proj, features.5.3.attn.qkv, features.5.4.attn.proj, features.5.4.attn.qkv, features.5.5.attn.proj, features.5.5.attn.qkv, features.7.0.attn.proj, features.7.0.attn.qkv, features.7.1.attn.proj, features.7.1.attn.qkv\n",
      "02/04 17:37:28 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Unsupported operator aten::layer_norm encountered 29 time(s)\n",
      "02/04 17:37:28 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Unsupported operator aten::adaptive_avg_pool2d encountered 1 time(s)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "+-------------------------+----------------------+------------+--------------+\n",
      "|\u001b[1m \u001b[0m\u001b[1mmodule                 \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m#parameters or shape\u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m#flops    \u001b[0m\u001b[1m \u001b[0m|\u001b[1m \u001b[0m\u001b[1m#activations\u001b[0m\u001b[1m \u001b[0m|\n",
      "+-------------------------+----------------------+------------+--------------+\n",
      "| model                   | 28.352M              | 4.956G     | 20.913M      |\n",
      "|  features               |  27.581M             |  4.955G    |  20.912M     |\n",
      "|   features.0            |   4.896K             |   15.956M  |   0.301M     |\n",
      "|    features.0.0         |    4.704K            |    14.451M |    0.301M    |\n",
      "|    features.0.2         |    0.192K            |    1.505M  |    0         |\n",
      "|   features.1            |   0.23M              |   0.778G   |   7.457M     |\n",
      "|    features.1.0         |    0.115M            |    0.389G  |    3.729M    |\n",
      "|    features.1.1         |    0.115M            |    0.389G  |    3.729M    |\n",
      "|   features.2            |   74.112K            |   58.555M  |   0.151M     |\n",
      "|    features.2.reduction |    73.728K           |    57.803M |    0.151M    |\n",
      "|    features.2.norm      |    0.384K            |    0.753M  |    0         |\n",
      "|   features.3            |   0.899M             |   0.82G    |   4.491M     |\n",
      "|    features.3.0         |    0.449M            |    0.41G   |    2.245M    |\n",
      "|    features.3.1         |    0.449M            |    0.41G   |    2.245M    |\n",
      "|   features.4            |   0.296M             |   58.179M  |   75.264K    |\n",
      "|    features.4.reduction |    0.295M            |    57.803M |    75.264K   |\n",
      "|    features.4.norm      |    0.768K            |    0.376M  |    0         |\n",
      "|   features.5            |   10.693M            |   2.383G   |   7.094M     |\n",
      "|    features.5.0         |    1.782M            |    0.397G  |    1.182M    |\n",
      "|    features.5.1         |    1.782M            |    0.397G  |    1.182M    |\n",
      "|    features.5.2         |    1.782M            |    0.397G  |    1.182M    |\n",
      "|    features.5.3         |    1.782M            |    0.397G  |    1.182M    |\n",
      "|    features.5.4         |    1.782M            |    0.397G  |    1.182M    |\n",
      "|    features.5.5         |    1.782M            |    0.397G  |    1.182M    |\n",
      "|   features.6            |   1.181M             |   57.991M  |   37.632K    |\n",
      "|    features.6.reduction |    1.18M             |    57.803M |    37.632K   |\n",
      "|    features.6.norm      |    1.536K            |    0.188M  |    0         |\n",
      "|   features.7            |   14.203M            |   0.784G   |   1.306M     |\n",
      "|    features.7.0         |    7.102M            |    0.392G  |    0.653M    |\n",
      "|    features.7.1         |    7.102M            |    0.392G  |    0.653M    |\n",
      "|  norm                   |  1.536K              |  0.188M    |  0           |\n",
      "|   norm.weight           |   (768,)             |            |              |\n",
      "|   norm.bias             |   (768,)             |            |              |\n",
      "|  head                   |  0.769M              |  0.768M    |  1K          |\n",
      "|   head.weight           |   (1000, 768)        |            |              |\n",
      "|   head.bias             |   (1000,)            |            |              |\n",
      "|  avgpool                |                      |  37.632K   |  0           |\n",
      "+-------------------------+----------------------+------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summarize_model(swin_v2_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's build a Model!\n",
    "\n",
    "We'll wrap a model from `torchvision` in MMEngine's `BaseModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try the smallest ViT we can get, a ViT-B16.\n",
    "vit_b_16 = torchvision.models.vit_b_16()\n",
    "\n",
    "class MMViT_B16(BaseModel):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, imgs, labels, mode):\n",
    "        x = self.model(imgs)\n",
    "        if mode == 'loss':\n",
    "            return {'loss': F.cross_entropy(x, labels)}\n",
    "        elif mode == 'predict':\n",
    "            return x, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50 = torchvision.models.resnet50()\n",
    "\n",
    "class MMResNet50(BaseModel):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, imgs, labels, mode):\n",
    "        x = self.model(imgs)\n",
    "        if mode == 'loss':\n",
    "            return {'loss': F.cross_entropy(x, labels)}\n",
    "        elif mode == 'predict':\n",
    "            return x, labels\n",
    "\n",
    "model = MMResNet50(resnet50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18 = torchvision.models.resnet18()\n",
    "\n",
    "class ModelWrapper(BaseModel):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, imgs, labels, mode):\n",
    "        x = self.model(imgs)\n",
    "        if mode == 'loss':\n",
    "            return {'loss': F.cross_entropy(x, labels)}\n",
    "        elif mode == 'predict':\n",
    "            return x, labels\n",
    "\n",
    "model = ModelWrapper(resnet18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's get some data!\n",
    "\n",
    "We need to create a `Dataset` and `DataLoader` for training and validation.\n",
    "Let's grab a dataset from `torchvision`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/evan/.pyenv/versions/3.11.7/envs/3117/lib/python3.11/site-packages/torchvision/transforms/v2/_deprecated.py:41: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to /home/evan/Data/cifar10/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 170498071/170498071 [00:19<00:00, 8547401.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /home/evan/Data/cifar10/cifar-10-python.tar.gz to /home/evan/Data/cifar10\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# We'll be going for CIFAR10.\n",
    "\n",
    "dpath = Path().home() / 'Data' / 'cifar10'\n",
    "\n",
    "# First, let's define the transforms.\n",
    "norm_cfg = dict(\n",
    "    mean=[0.491, 0.482, 0.447],\n",
    "    std=[0.202, 0.199, 0.201]\n",
    ")\n",
    "\n",
    "# ---------------------------------  train  --------------------------------- #\n",
    "\n",
    "train_transforms = T.Compose([\n",
    "    T.RandomCrop(32, padding=4),\n",
    "    T.RandomHorizontalFlip(),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(**norm_cfg),\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(\n",
    "    dpath,\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=train_transforms,\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(batch_size=32, shuffle=True, dataset=train_dataset)\n",
    "\n",
    "# ----------------------------------  val  ---------------------------------- #\n",
    "\n",
    "val_transforms = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(**norm_cfg),\n",
    "])\n",
    "\n",
    "val_dataset = torchvision.datasets.CIFAR10(\n",
    "    dpath,\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=val_transforms,\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(batch_size=32, shuffle=False, dataset=val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build an Evaluation Metric\n",
    "To evaluate model performance, we need to define a **Metric** called accuracy. \n",
    "This metric needs inherit from `BaseMetric` and implement the \n",
    "`process` and `compute_metrics` methods.\n",
    "\n",
    "`process`: \n",
    "This method receives a batch of data from the dataloader and outputs from the\n",
    "model (in `mode=\"predict\"`). \n",
    "After processing this data, we save the information to `self.results` property.\n",
    "\n",
    "```{note}\n",
    "In a distributed environment, `self.results` is the information collected \n",
    "from all the processes.\n",
    "```\n",
    "\n",
    "`compute_metrics`:\n",
    "This method receives the `results` processed by `process` to calculate an\n",
    "evaluation metric, returning the information as a `dict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmengine.evaluator import BaseMetric\n",
    "\n",
    "class Accuracy(BaseMetric):\n",
    "    \n",
    "    def process(self, data_batch, data_samples):\n",
    "        score, gt = data_samples\n",
    "        # Save the middle result of a batch to `self.results`\n",
    "        self.results.append({\n",
    "            'batch_size': len(gt),\n",
    "            'correct': (score.argmax(dim=1) == gt).sum().cpu(),\n",
    "        })\n",
    "\n",
    "    def compute_metrics(self, results):\n",
    "        total_correct = sum(item['correct'] for item in results)\n",
    "        total_size = sum(item['batch_size'] for item in results)\n",
    "        accuracy = 100 * total_correct / total_size\n",
    "        # The key is the name of the metric.\n",
    "        return dict(accuracy=accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Runner and SEND IT 🤙🏻\n",
    "\n",
    "Now we can build a Runner with previously defined Model, DataLoader, and Metrics, and some other configs shown as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work_dir = Path().home() / 'Experiments' / 'ShibaTown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02/05 22:51:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "------------------------------------------------------------\n",
      "System environment:\n",
      "    sys.platform: linux\n",
      "    Python: 3.11.7 (main, Dec 20 2023, 09:04:31) [GCC 12.3.0]\n",
      "    CUDA available: True\n",
      "    MUSA available: False\n",
      "    numpy_random_seed: 1743727311\n",
      "    GPU 0: NVIDIA RTX A500 Laptop GPU\n",
      "    CUDA_HOME: /usr/local/cuda\n",
      "    NVCC: Cuda compilation tools, release 12.3, V12.3.107\n",
      "    GCC: gcc (Ubuntu 12.3.0-1ubuntu1~22.04) 12.3.0\n",
      "    PyTorch: 2.2.0+cu121\n",
      "    PyTorch compiling details: PyTorch built with:\n",
      "  - GCC 9.3\n",
      "  - C++ Version: 201703\n",
      "  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications\n",
      "  - Intel(R) MKL-DNN v3.3.2 (Git Hash 2dc95a2ad0841e29db8b22fbccaf3e5da7992b01)\n",
      "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
      "  - LAPACK is enabled (usually provided by MKL)\n",
      "  - NNPACK is enabled\n",
      "  - CPU capability usage: AVX2\n",
      "  - CUDA Runtime 12.1\n",
      "  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90\n",
      "  - CuDNN 8.9.2\n",
      "  - Magma 2.6.1\n",
      "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.2.0, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, \n",
      "\n",
      "    TorchVision: 0.17.0+cu121\n",
      "    OpenCV: 4.9.0\n",
      "    MMEngine: 0.10.3\n",
      "\n",
      "Runtime environment:\n",
      "    dist_cfg: {'backend': 'nccl'}\n",
      "    seed: 1743727311\n",
      "    Distributed launcher: none\n",
      "    Distributed training: False\n",
      "    GPU number: 1\n",
      "------------------------------------------------------------\n",
      "\n",
      "02/05 22:51:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.\n",
      "02/05 22:51:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "before_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) DistSamplerSeedHook                \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_val:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_val:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "after_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_test:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "before_test_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      " -------------------- \n",
      "after_run:\n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "02/05 22:51:06 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Dataset CIFAR10 has no metainfo. ``dataset_meta`` in visualizer will be None.\n",
      "02/05 22:51:06 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - The prefix is not set in metric class Accuracy.\n",
      "02/05 22:51:06 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Dataset CIFAR10 has no metainfo. ``dataset_meta`` in evaluator, metric and visualizer will be None.\n",
      "02/05 22:51:06 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"FileClient\" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io\n",
      "02/05 22:51:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Checkpoints will be saved to /home/evan/Experiments/ShibaTown.\n",
      "02/05 22:51:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][  10/1563]  lr: 1.0000e-03  eta: 0:01:12  time: 0.0231  data_time: 0.0068  memory: 837  loss: 6.8060\n",
      "02/05 22:51:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][  20/1563]  lr: 1.0000e-03  eta: 0:01:05  time: 0.0188  data_time: 0.0050  memory: 693  loss: 4.7651\n",
      "02/05 22:51:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][  30/1563]  lr: 1.0000e-03  eta: 0:01:02  time: 0.0188  data_time: 0.0049  memory: 693  loss: 2.9700\n",
      "02/05 22:51:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][  40/1563]  lr: 1.0000e-03  eta: 0:01:01  time: 0.0187  data_time: 0.0049  memory: 693  loss: 2.4523\n",
      "02/05 22:51:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][  50/1563]  lr: 1.0000e-03  eta: 0:01:00  time: 0.0195  data_time: 0.0051  memory: 693  loss: 2.2278\n",
      "02/05 22:51:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][  60/1563]  lr: 1.0000e-03  eta: 0:01:00  time: 0.0190  data_time: 0.0050  memory: 693  loss: 2.1723\n",
      "02/05 22:51:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][  70/1563]  lr: 1.0000e-03  eta: 0:00:59  time: 0.0195  data_time: 0.0055  memory: 693  loss: 2.1316\n",
      "02/05 22:51:08 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][  80/1563]  lr: 1.0000e-03  eta: 0:00:59  time: 0.0193  data_time: 0.0054  memory: 693  loss: 2.0914\n",
      "02/05 22:51:08 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][  90/1563]  lr: 1.0000e-03  eta: 0:00:59  time: 0.0188  data_time: 0.0050  memory: 693  loss: 2.0392\n",
      "02/05 22:51:08 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 100/1563]  lr: 1.0000e-03  eta: 0:00:59  time: 0.0196  data_time: 0.0054  memory: 693  loss: 2.0431\n",
      "02/05 22:51:08 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 110/1563]  lr: 1.0000e-03  eta: 0:00:58  time: 0.0197  data_time: 0.0056  memory: 693  loss: 2.0259\n",
      "02/05 22:51:08 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 120/1563]  lr: 1.0000e-03  eta: 0:00:58  time: 0.0199  data_time: 0.0058  memory: 693  loss: 2.0115\n",
      "02/05 22:51:09 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 130/1563]  lr: 1.0000e-03  eta: 0:00:58  time: 0.0196  data_time: 0.0056  memory: 693  loss: 1.9202\n",
      "02/05 22:51:09 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 140/1563]  lr: 1.0000e-03  eta: 0:00:58  time: 0.0188  data_time: 0.0049  memory: 693  loss: 1.9363\n",
      "02/05 22:51:09 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 150/1563]  lr: 1.0000e-03  eta: 0:00:57  time: 0.0192  data_time: 0.0051  memory: 693  loss: 2.0259\n",
      "02/05 22:51:09 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 160/1563]  lr: 1.0000e-03  eta: 0:00:57  time: 0.0194  data_time: 0.0053  memory: 693  loss: 1.9841\n",
      "02/05 22:51:09 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 170/1563]  lr: 1.0000e-03  eta: 0:00:57  time: 0.0195  data_time: 0.0054  memory: 693  loss: 2.0606\n",
      "02/05 22:51:09 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 180/1563]  lr: 1.0000e-03  eta: 0:00:57  time: 0.0192  data_time: 0.0052  memory: 693  loss: 2.0006\n",
      "02/05 22:51:10 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 190/1563]  lr: 1.0000e-03  eta: 0:00:57  time: 0.0192  data_time: 0.0053  memory: 693  loss: 1.8945\n",
      "02/05 22:51:10 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 200/1563]  lr: 1.0000e-03  eta: 0:00:56  time: 0.0194  data_time: 0.0054  memory: 693  loss: 1.9755\n",
      "02/05 22:51:10 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 210/1563]  lr: 1.0000e-03  eta: 0:00:56  time: 0.0198  data_time: 0.0055  memory: 693  loss: 1.8683\n",
      "02/05 22:51:10 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 220/1563]  lr: 1.0000e-03  eta: 0:00:56  time: 0.0191  data_time: 0.0050  memory: 693  loss: 1.9332\n",
      "02/05 22:51:10 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 230/1563]  lr: 1.0000e-03  eta: 0:00:56  time: 0.0194  data_time: 0.0054  memory: 693  loss: 1.7716\n",
      "02/05 22:51:11 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 240/1563]  lr: 1.0000e-03  eta: 0:00:56  time: 0.0190  data_time: 0.0050  memory: 693  loss: 1.9252\n",
      "02/05 22:51:11 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 250/1563]  lr: 1.0000e-03  eta: 0:00:55  time: 0.0197  data_time: 0.0056  memory: 693  loss: 1.9457\n",
      "02/05 22:51:11 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 260/1563]  lr: 1.0000e-03  eta: 0:00:55  time: 0.0200  data_time: 0.0060  memory: 693  loss: 1.8722\n",
      "02/05 22:51:11 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 270/1563]  lr: 1.0000e-03  eta: 0:00:55  time: 0.0193  data_time: 0.0054  memory: 693  loss: 1.9505\n",
      "02/05 22:51:11 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 280/1563]  lr: 1.0000e-03  eta: 0:00:55  time: 0.0192  data_time: 0.0051  memory: 693  loss: 1.7709\n",
      "02/05 22:51:12 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 290/1563]  lr: 1.0000e-03  eta: 0:00:55  time: 0.0190  data_time: 0.0051  memory: 693  loss: 1.8652\n",
      "02/05 22:51:12 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 300/1563]  lr: 1.0000e-03  eta: 0:00:54  time: 0.0197  data_time: 0.0058  memory: 693  loss: 1.8613\n",
      "02/05 22:51:12 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 310/1563]  lr: 1.0000e-03  eta: 0:00:54  time: 0.0197  data_time: 0.0055  memory: 693  loss: 1.8652\n",
      "02/05 22:51:12 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 320/1563]  lr: 1.0000e-03  eta: 0:00:54  time: 0.0192  data_time: 0.0052  memory: 693  loss: 1.8211\n",
      "02/05 22:51:12 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 330/1563]  lr: 1.0000e-03  eta: 0:00:54  time: 0.0189  data_time: 0.0050  memory: 693  loss: 1.8570\n",
      "02/05 22:51:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 340/1563]  lr: 1.0000e-03  eta: 0:00:54  time: 0.0190  data_time: 0.0049  memory: 693  loss: 1.7934\n",
      "02/05 22:51:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 350/1563]  lr: 1.0000e-03  eta: 0:00:53  time: 0.0192  data_time: 0.0052  memory: 693  loss: 1.9325\n",
      "02/05 22:51:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 360/1563]  lr: 1.0000e-03  eta: 0:00:53  time: 0.0190  data_time: 0.0049  memory: 693  loss: 1.8701\n",
      "02/05 22:51:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 370/1563]  lr: 1.0000e-03  eta: 0:00:53  time: 0.0192  data_time: 0.0052  memory: 693  loss: 1.7172\n",
      "02/05 22:51:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 380/1563]  lr: 1.0000e-03  eta: 0:00:53  time: 0.0201  data_time: 0.0062  memory: 693  loss: 1.8447\n",
      "02/05 22:51:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 390/1563]  lr: 1.0000e-03  eta: 0:00:53  time: 0.0190  data_time: 0.0049  memory: 693  loss: 1.8563\n",
      "02/05 22:51:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 400/1563]  lr: 1.0000e-03  eta: 0:00:52  time: 0.0190  data_time: 0.0049  memory: 693  loss: 1.7907\n",
      "02/05 22:51:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 410/1563]  lr: 1.0000e-03  eta: 0:00:52  time: 0.0191  data_time: 0.0050  memory: 693  loss: 1.8278\n",
      "02/05 22:51:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 420/1563]  lr: 1.0000e-03  eta: 0:00:52  time: 0.0195  data_time: 0.0054  memory: 693  loss: 1.7670\n",
      "02/05 22:51:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 430/1563]  lr: 1.0000e-03  eta: 0:00:52  time: 0.0192  data_time: 0.0053  memory: 693  loss: 1.7377\n",
      "02/05 22:51:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 440/1563]  lr: 1.0000e-03  eta: 0:00:52  time: 0.0194  data_time: 0.0053  memory: 693  loss: 1.7842\n",
      "02/05 22:51:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 450/1563]  lr: 1.0000e-03  eta: 0:00:51  time: 0.0190  data_time: 0.0049  memory: 693  loss: 1.8509\n",
      "02/05 22:51:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 460/1563]  lr: 1.0000e-03  eta: 0:00:51  time: 0.0189  data_time: 0.0048  memory: 693  loss: 1.7760\n",
      "02/05 22:51:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 470/1563]  lr: 1.0000e-03  eta: 0:00:51  time: 0.0194  data_time: 0.0052  memory: 693  loss: 1.7237\n",
      "02/05 22:51:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 480/1563]  lr: 1.0000e-03  eta: 0:00:51  time: 0.0192  data_time: 0.0052  memory: 693  loss: 1.7283\n",
      "02/05 22:51:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 490/1563]  lr: 1.0000e-03  eta: 0:00:51  time: 0.0190  data_time: 0.0050  memory: 693  loss: 1.8292\n",
      "02/05 22:51:16 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 500/1563]  lr: 1.0000e-03  eta: 0:00:50  time: 0.0191  data_time: 0.0050  memory: 693  loss: 1.7913\n",
      "02/05 22:51:16 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 510/1563]  lr: 1.0000e-03  eta: 0:00:50  time: 0.0190  data_time: 0.0049  memory: 693  loss: 1.8215\n",
      "02/05 22:51:16 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 520/1563]  lr: 1.0000e-03  eta: 0:00:50  time: 0.0196  data_time: 0.0053  memory: 693  loss: 1.7300\n",
      "02/05 22:51:16 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 530/1563]  lr: 1.0000e-03  eta: 0:00:50  time: 0.0192  data_time: 0.0052  memory: 693  loss: 1.6605\n",
      "02/05 22:51:16 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 540/1563]  lr: 1.0000e-03  eta: 0:00:50  time: 0.0190  data_time: 0.0050  memory: 693  loss: 1.7316\n",
      "02/05 22:51:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 550/1563]  lr: 1.0000e-03  eta: 0:00:49  time: 0.0192  data_time: 0.0051  memory: 693  loss: 1.7965\n",
      "02/05 22:51:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 560/1563]  lr: 1.0000e-03  eta: 0:00:49  time: 0.0191  data_time: 0.0050  memory: 693  loss: 1.6773\n",
      "02/05 22:51:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 570/1563]  lr: 1.0000e-03  eta: 0:00:49  time: 0.0195  data_time: 0.0053  memory: 693  loss: 1.7069\n",
      "02/05 22:51:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 580/1563]  lr: 1.0000e-03  eta: 0:00:49  time: 0.0203  data_time: 0.0063  memory: 693  loss: 1.7658\n",
      "02/05 22:51:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 590/1563]  lr: 1.0000e-03  eta: 0:00:49  time: 0.0190  data_time: 0.0050  memory: 693  loss: 1.8202\n",
      "02/05 22:51:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 600/1563]  lr: 1.0000e-03  eta: 0:00:48  time: 0.0190  data_time: 0.0049  memory: 693  loss: 1.7408\n",
      "02/05 22:51:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 610/1563]  lr: 1.0000e-03  eta: 0:00:48  time: 0.0193  data_time: 0.0052  memory: 693  loss: 1.7441\n",
      "02/05 22:51:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 620/1563]  lr: 1.0000e-03  eta: 0:00:48  time: 0.0191  data_time: 0.0050  memory: 693  loss: 1.7232\n",
      "02/05 22:51:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 630/1563]  lr: 1.0000e-03  eta: 0:00:48  time: 0.0192  data_time: 0.0050  memory: 693  loss: 1.6306\n",
      "02/05 22:51:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 640/1563]  lr: 1.0000e-03  eta: 0:00:48  time: 0.0190  data_time: 0.0050  memory: 693  loss: 1.7014\n",
      "02/05 22:51:19 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 650/1563]  lr: 1.0000e-03  eta: 0:00:47  time: 0.0193  data_time: 0.0052  memory: 693  loss: 1.8188\n",
      "02/05 22:51:19 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 660/1563]  lr: 1.0000e-03  eta: 0:00:47  time: 0.0191  data_time: 0.0050  memory: 693  loss: 1.7702\n",
      "02/05 22:51:19 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 670/1563]  lr: 1.0000e-03  eta: 0:00:47  time: 0.0193  data_time: 0.0052  memory: 693  loss: 1.7684\n",
      "02/05 22:51:19 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 680/1563]  lr: 1.0000e-03  eta: 0:00:47  time: 0.0192  data_time: 0.0050  memory: 693  loss: 1.6782\n",
      "02/05 22:51:19 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 690/1563]  lr: 1.0000e-03  eta: 0:00:47  time: 0.0191  data_time: 0.0050  memory: 693  loss: 1.7081\n",
      "02/05 22:51:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 700/1563]  lr: 1.0000e-03  eta: 0:00:46  time: 0.0193  data_time: 0.0052  memory: 693  loss: 1.7983\n",
      "02/05 22:51:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 710/1563]  lr: 1.0000e-03  eta: 0:00:46  time: 0.0193  data_time: 0.0051  memory: 693  loss: 1.6966\n",
      "02/05 22:51:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 720/1563]  lr: 1.0000e-03  eta: 0:00:46  time: 0.0193  data_time: 0.0052  memory: 693  loss: 1.7218\n",
      "02/05 22:51:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 730/1563]  lr: 1.0000e-03  eta: 0:00:46  time: 0.0192  data_time: 0.0050  memory: 693  loss: 1.6534\n",
      "02/05 22:51:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 740/1563]  lr: 1.0000e-03  eta: 0:00:46  time: 0.0195  data_time: 0.0053  memory: 693  loss: 1.6617\n",
      "02/05 22:51:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 750/1563]  lr: 1.0000e-03  eta: 0:00:45  time: 0.0193  data_time: 0.0052  memory: 693  loss: 1.7483\n",
      "02/05 22:51:21 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 760/1563]  lr: 1.0000e-03  eta: 0:00:45  time: 0.0192  data_time: 0.0050  memory: 693  loss: 1.5903\n",
      "02/05 22:51:21 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 770/1563]  lr: 1.0000e-03  eta: 0:00:45  time: 0.0195  data_time: 0.0053  memory: 693  loss: 1.5955\n",
      "02/05 22:51:21 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 780/1563]  lr: 1.0000e-03  eta: 0:00:45  time: 0.0197  data_time: 0.0055  memory: 693  loss: 1.6120\n",
      "02/05 22:51:21 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 790/1563]  lr: 1.0000e-03  eta: 0:00:45  time: 0.0190  data_time: 0.0049  memory: 693  loss: 1.7105\n",
      "02/05 22:51:21 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 800/1563]  lr: 1.0000e-03  eta: 0:00:44  time: 0.0192  data_time: 0.0050  memory: 693  loss: 1.7097\n",
      "02/05 22:51:22 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 810/1563]  lr: 1.0000e-03  eta: 0:00:44  time: 0.0191  data_time: 0.0051  memory: 693  loss: 1.6981\n",
      "02/05 22:51:22 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 820/1563]  lr: 1.0000e-03  eta: 0:00:44  time: 0.0195  data_time: 0.0053  memory: 693  loss: 1.6448\n",
      "02/05 22:51:22 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 830/1563]  lr: 1.0000e-03  eta: 0:00:44  time: 0.0192  data_time: 0.0050  memory: 693  loss: 1.6612\n",
      "02/05 22:51:22 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 840/1563]  lr: 1.0000e-03  eta: 0:00:44  time: 0.0191  data_time: 0.0050  memory: 693  loss: 1.6152\n",
      "02/05 22:51:22 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 850/1563]  lr: 1.0000e-03  eta: 0:00:44  time: 0.0208  data_time: 0.0068  memory: 693  loss: 1.6175\n",
      "02/05 22:51:23 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 860/1563]  lr: 1.0000e-03  eta: 0:00:43  time: 0.0197  data_time: 0.0056  memory: 693  loss: 1.6678\n",
      "02/05 22:51:23 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 870/1563]  lr: 1.0000e-03  eta: 0:00:43  time: 0.0192  data_time: 0.0050  memory: 693  loss: 1.6899\n",
      "02/05 22:51:23 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 880/1563]  lr: 1.0000e-03  eta: 0:00:43  time: 0.0191  data_time: 0.0049  memory: 693  loss: 1.5727\n",
      "02/05 22:51:23 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 890/1563]  lr: 1.0000e-03  eta: 0:00:43  time: 0.0192  data_time: 0.0051  memory: 693  loss: 1.6313\n",
      "02/05 22:51:23 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 900/1563]  lr: 1.0000e-03  eta: 0:00:43  time: 0.0193  data_time: 0.0051  memory: 693  loss: 1.5633\n",
      "02/05 22:51:24 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 910/1563]  lr: 1.0000e-03  eta: 0:00:42  time: 0.0191  data_time: 0.0050  memory: 693  loss: 1.5594\n",
      "02/05 22:51:24 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 920/1563]  lr: 1.0000e-03  eta: 0:00:42  time: 0.0192  data_time: 0.0051  memory: 693  loss: 1.6583\n",
      "02/05 22:51:24 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 930/1563]  lr: 1.0000e-03  eta: 0:00:42  time: 0.0194  data_time: 0.0052  memory: 693  loss: 1.5989\n",
      "02/05 22:51:24 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 940/1563]  lr: 1.0000e-03  eta: 0:00:42  time: 0.0191  data_time: 0.0050  memory: 693  loss: 1.5563\n",
      "02/05 22:51:24 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 950/1563]  lr: 1.0000e-03  eta: 0:00:42  time: 0.0191  data_time: 0.0049  memory: 693  loss: 1.6146\n",
      "02/05 22:51:25 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 960/1563]  lr: 1.0000e-03  eta: 0:00:41  time: 0.0190  data_time: 0.0049  memory: 693  loss: 1.6009\n",
      "02/05 22:51:25 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 970/1563]  lr: 1.0000e-03  eta: 0:00:41  time: 0.0191  data_time: 0.0049  memory: 693  loss: 1.5944\n",
      "02/05 22:51:25 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 980/1563]  lr: 1.0000e-03  eta: 0:00:41  time: 0.0195  data_time: 0.0052  memory: 693  loss: 1.5273\n",
      "02/05 22:51:25 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 990/1563]  lr: 1.0000e-03  eta: 0:00:41  time: 0.0192  data_time: 0.0050  memory: 693  loss: 1.6480\n",
      "02/05 22:51:25 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: 20240205_225106\n",
      "02/05 22:51:25 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1000/1563]  lr: 1.0000e-03  eta: 0:00:41  time: 0.0197  data_time: 0.0055  memory: 693  loss: 1.5931\n",
      "02/05 22:51:26 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1010/1563]  lr: 1.0000e-03  eta: 0:00:40  time: 0.0193  data_time: 0.0051  memory: 693  loss: 1.6793\n",
      "02/05 22:51:26 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1020/1563]  lr: 1.0000e-03  eta: 0:00:40  time: 0.0191  data_time: 0.0050  memory: 693  loss: 1.6560\n",
      "02/05 22:51:26 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1030/1563]  lr: 1.0000e-03  eta: 0:00:40  time: 0.0192  data_time: 0.0050  memory: 693  loss: 1.6359\n",
      "02/05 22:51:26 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1040/1563]  lr: 1.0000e-03  eta: 0:00:40  time: 0.0194  data_time: 0.0052  memory: 693  loss: 1.6200\n",
      "02/05 22:51:26 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1050/1563]  lr: 1.0000e-03  eta: 0:00:40  time: 0.0191  data_time: 0.0051  memory: 693  loss: 1.6227\n",
      "02/05 22:51:26 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1060/1563]  lr: 1.0000e-03  eta: 0:00:39  time: 0.0192  data_time: 0.0050  memory: 693  loss: 1.5052\n",
      "02/05 22:51:27 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1070/1563]  lr: 1.0000e-03  eta: 0:00:39  time: 0.0192  data_time: 0.0050  memory: 693  loss: 1.6782\n",
      "02/05 22:51:27 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1080/1563]  lr: 1.0000e-03  eta: 0:00:39  time: 0.0191  data_time: 0.0049  memory: 693  loss: 1.6602\n",
      "02/05 22:51:27 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1090/1563]  lr: 1.0000e-03  eta: 0:00:39  time: 0.0193  data_time: 0.0050  memory: 693  loss: 1.6942\n",
      "02/05 22:51:27 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1100/1563]  lr: 1.0000e-03  eta: 0:00:39  time: 0.0191  data_time: 0.0050  memory: 693  loss: 1.5684\n",
      "02/05 22:51:27 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1110/1563]  lr: 1.0000e-03  eta: 0:00:38  time: 0.0192  data_time: 0.0050  memory: 693  loss: 1.5202\n",
      "02/05 22:51:28 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1120/1563]  lr: 1.0000e-03  eta: 0:00:38  time: 0.0192  data_time: 0.0050  memory: 693  loss: 1.6417\n",
      "02/05 22:51:28 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1130/1563]  lr: 1.0000e-03  eta: 0:00:38  time: 0.0192  data_time: 0.0050  memory: 693  loss: 1.5427\n",
      "02/05 22:51:28 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1140/1563]  lr: 1.0000e-03  eta: 0:00:38  time: 0.0193  data_time: 0.0050  memory: 693  loss: 1.6787\n",
      "02/05 22:51:28 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1150/1563]  lr: 1.0000e-03  eta: 0:00:38  time: 0.0192  data_time: 0.0050  memory: 693  loss: 1.6311\n",
      "02/05 22:51:28 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1160/1563]  lr: 1.0000e-03  eta: 0:00:37  time: 0.0192  data_time: 0.0049  memory: 693  loss: 1.5930\n",
      "02/05 22:51:29 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1170/1563]  lr: 1.0000e-03  eta: 0:00:37  time: 0.0198  data_time: 0.0056  memory: 693  loss: 1.6603\n",
      "02/05 22:51:29 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1180/1563]  lr: 1.0000e-03  eta: 0:00:37  time: 0.0192  data_time: 0.0050  memory: 693  loss: 1.5246\n",
      "02/05 22:51:29 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1190/1563]  lr: 1.0000e-03  eta: 0:00:37  time: 0.0192  data_time: 0.0049  memory: 693  loss: 1.6674\n",
      "02/05 22:51:29 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1200/1563]  lr: 1.0000e-03  eta: 0:00:37  time: 0.0192  data_time: 0.0050  memory: 693  loss: 1.5866\n",
      "02/05 22:51:29 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1210/1563]  lr: 1.0000e-03  eta: 0:00:36  time: 0.0192  data_time: 0.0050  memory: 693  loss: 1.5948\n",
      "02/05 22:51:30 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1220/1563]  lr: 1.0000e-03  eta: 0:00:36  time: 0.0193  data_time: 0.0050  memory: 693  loss: 1.6367\n",
      "02/05 22:51:30 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1230/1563]  lr: 1.0000e-03  eta: 0:00:36  time: 0.0192  data_time: 0.0050  memory: 693  loss: 1.5551\n",
      "02/05 22:51:30 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1240/1563]  lr: 1.0000e-03  eta: 0:00:36  time: 0.0192  data_time: 0.0049  memory: 693  loss: 1.6899\n",
      "02/05 22:51:30 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1250/1563]  lr: 1.0000e-03  eta: 0:00:36  time: 0.0193  data_time: 0.0050  memory: 693  loss: 1.4839\n",
      "02/05 22:51:30 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1260/1563]  lr: 1.0000e-03  eta: 0:00:36  time: 0.0197  data_time: 0.0056  memory: 693  loss: 1.5979\n",
      "02/05 22:51:31 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1270/1563]  lr: 1.0000e-03  eta: 0:00:35  time: 0.0191  data_time: 0.0049  memory: 693  loss: 1.6384\n",
      "02/05 22:51:31 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1280/1563]  lr: 1.0000e-03  eta: 0:00:35  time: 0.0193  data_time: 0.0050  memory: 693  loss: 1.5748\n",
      "02/05 22:51:31 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1290/1563]  lr: 1.0000e-03  eta: 0:00:35  time: 0.0192  data_time: 0.0050  memory: 693  loss: 1.6613\n",
      "02/05 22:51:31 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1300/1563]  lr: 1.0000e-03  eta: 0:00:35  time: 0.0192  data_time: 0.0050  memory: 693  loss: 1.4912\n",
      "02/05 22:51:31 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1310/1563]  lr: 1.0000e-03  eta: 0:00:35  time: 0.0191  data_time: 0.0049  memory: 693  loss: 1.6301\n",
      "02/05 22:51:31 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1320/1563]  lr: 1.0000e-03  eta: 0:00:34  time: 0.0190  data_time: 0.0048  memory: 693  loss: 1.5775\n",
      "02/05 22:51:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1330/1563]  lr: 1.0000e-03  eta: 0:00:34  time: 0.0193  data_time: 0.0050  memory: 693  loss: 1.6024\n",
      "02/05 22:51:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1340/1563]  lr: 1.0000e-03  eta: 0:00:34  time: 0.0191  data_time: 0.0049  memory: 693  loss: 1.6344\n",
      "02/05 22:51:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1350/1563]  lr: 1.0000e-03  eta: 0:00:34  time: 0.0192  data_time: 0.0049  memory: 693  loss: 1.4908\n",
      "02/05 22:51:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1360/1563]  lr: 1.0000e-03  eta: 0:00:34  time: 0.0192  data_time: 0.0050  memory: 693  loss: 1.5807\n",
      "02/05 22:51:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1370/1563]  lr: 1.0000e-03  eta: 0:00:33  time: 0.0192  data_time: 0.0050  memory: 693  loss: 1.5321\n",
      "02/05 22:51:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1380/1563]  lr: 1.0000e-03  eta: 0:00:33  time: 0.0193  data_time: 0.0050  memory: 693  loss: 1.6478\n",
      "02/05 22:51:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1390/1563]  lr: 1.0000e-03  eta: 0:00:33  time: 0.0192  data_time: 0.0050  memory: 693  loss: 1.5458\n",
      "02/05 22:51:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1400/1563]  lr: 1.0000e-03  eta: 0:00:33  time: 0.0193  data_time: 0.0050  memory: 693  loss: 1.6503\n",
      "02/05 22:51:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1410/1563]  lr: 1.0000e-03  eta: 0:00:33  time: 0.0192  data_time: 0.0050  memory: 693  loss: 1.5718\n",
      "02/05 22:51:33 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1420/1563]  lr: 1.0000e-03  eta: 0:00:32  time: 0.0191  data_time: 0.0049  memory: 693  loss: 1.6338\n",
      "02/05 22:51:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1430/1563]  lr: 1.0000e-03  eta: 0:00:32  time: 0.0193  data_time: 0.0049  memory: 693  loss: 1.5776\n",
      "02/05 22:51:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1440/1563]  lr: 1.0000e-03  eta: 0:00:32  time: 0.0192  data_time: 0.0049  memory: 693  loss: 1.5771\n",
      "02/05 22:51:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1450/1563]  lr: 1.0000e-03  eta: 0:00:32  time: 0.0192  data_time: 0.0050  memory: 693  loss: 1.4624\n",
      "02/05 22:51:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1460/1563]  lr: 1.0000e-03  eta: 0:00:32  time: 0.0192  data_time: 0.0049  memory: 693  loss: 1.5121\n",
      "02/05 22:51:34 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1470/1563]  lr: 1.0000e-03  eta: 0:00:31  time: 0.0194  data_time: 0.0052  memory: 693  loss: 1.6546\n",
      "02/05 22:51:35 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1480/1563]  lr: 1.0000e-03  eta: 0:00:31  time: 0.0197  data_time: 0.0055  memory: 693  loss: 1.5254\n",
      "02/05 22:51:35 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1490/1563]  lr: 1.0000e-03  eta: 0:00:31  time: 0.0199  data_time: 0.0057  memory: 693  loss: 1.4966\n",
      "02/05 22:51:35 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1500/1563]  lr: 1.0000e-03  eta: 0:00:31  time: 0.0198  data_time: 0.0057  memory: 693  loss: 1.5960\n",
      "02/05 22:51:35 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1510/1563]  lr: 1.0000e-03  eta: 0:00:31  time: 0.0201  data_time: 0.0060  memory: 693  loss: 1.4465\n",
      "02/05 22:51:35 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1520/1563]  lr: 1.0000e-03  eta: 0:00:31  time: 0.0201  data_time: 0.0060  memory: 693  loss: 1.5730\n",
      "02/05 22:51:36 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1530/1563]  lr: 1.0000e-03  eta: 0:00:30  time: 0.0200  data_time: 0.0059  memory: 693  loss: 1.5539\n",
      "02/05 22:51:36 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1540/1563]  lr: 1.0000e-03  eta: 0:00:30  time: 0.0202  data_time: 0.0061  memory: 693  loss: 1.6197\n",
      "02/05 22:51:36 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1550/1563]  lr: 1.0000e-03  eta: 0:00:30  time: 0.0201  data_time: 0.0061  memory: 693  loss: 1.5445\n",
      "02/05 22:51:36 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1560/1563]  lr: 1.0000e-03  eta: 0:00:30  time: 0.0203  data_time: 0.0062  memory: 693  loss: 1.4970\n",
      "02/05 22:51:36 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: 20240205_225106\n",
      "02/05 22:51:36 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 1 epochs\n",
      "02/05 22:51:36 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - `save_param_scheduler` is True but `self.param_schedulers` is None, so skip saving parameter schedulers\n",
      "02/05 22:51:37 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][ 10/313]    eta: 0:00:02  time: 0.0095  data_time: 0.0036  memory: 693  \n",
      "02/05 22:51:37 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][ 20/313]    eta: 0:00:02  time: 0.0071  data_time: 0.0037  memory: 644  \n",
      "02/05 22:51:37 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][ 30/313]    eta: 0:00:02  time: 0.0071  data_time: 0.0037  memory: 644  \n",
      "02/05 22:51:37 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][ 40/313]    eta: 0:00:02  time: 0.0072  data_time: 0.0037  memory: 644  \n",
      "02/05 22:51:37 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][ 50/313]    eta: 0:00:01  time: 0.0071  data_time: 0.0036  memory: 644  \n",
      "02/05 22:51:37 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][ 60/313]    eta: 0:00:01  time: 0.0071  data_time: 0.0037  memory: 644  \n",
      "02/05 22:51:37 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][ 70/313]    eta: 0:00:01  time: 0.0072  data_time: 0.0037  memory: 644  \n",
      "02/05 22:51:37 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][ 80/313]    eta: 0:00:01  time: 0.0072  data_time: 0.0037  memory: 644  \n",
      "02/05 22:51:37 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][ 90/313]    eta: 0:00:01  time: 0.0072  data_time: 0.0037  memory: 644  \n",
      "02/05 22:51:37 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][100/313]    eta: 0:00:01  time: 0.0072  data_time: 0.0038  memory: 644  \n",
      "02/05 22:51:37 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][110/313]    eta: 0:00:01  time: 0.0072  data_time: 0.0037  memory: 644  \n",
      "02/05 22:51:37 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][120/313]    eta: 0:00:01  time: 0.0072  data_time: 0.0037  memory: 644  \n",
      "02/05 22:51:37 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][130/313]    eta: 0:00:01  time: 0.0073  data_time: 0.0038  memory: 644  \n",
      "02/05 22:51:38 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][140/313]    eta: 0:00:01  time: 0.0072  data_time: 0.0038  memory: 644  \n",
      "02/05 22:51:38 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][150/313]    eta: 0:00:01  time: 0.0073  data_time: 0.0038  memory: 644  \n",
      "02/05 22:51:38 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][160/313]    eta: 0:00:01  time: 0.0073  data_time: 0.0038  memory: 644  \n",
      "02/05 22:51:38 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][170/313]    eta: 0:00:01  time: 0.0073  data_time: 0.0038  memory: 644  \n",
      "02/05 22:51:38 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][180/313]    eta: 0:00:00  time: 0.0072  data_time: 0.0037  memory: 644  \n",
      "02/05 22:51:38 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][190/313]    eta: 0:00:00  time: 0.0072  data_time: 0.0037  memory: 644  \n",
      "02/05 22:51:38 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][200/313]    eta: 0:00:00  time: 0.0072  data_time: 0.0038  memory: 644  \n",
      "02/05 22:51:38 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][210/313]    eta: 0:00:00  time: 0.0072  data_time: 0.0038  memory: 644  \n",
      "02/05 22:51:38 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][220/313]    eta: 0:00:00  time: 0.0072  data_time: 0.0038  memory: 644  \n",
      "02/05 22:51:38 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][230/313]    eta: 0:00:00  time: 0.0072  data_time: 0.0037  memory: 644  \n",
      "02/05 22:51:38 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][240/313]    eta: 0:00:00  time: 0.0073  data_time: 0.0038  memory: 644  \n",
      "02/05 22:51:38 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][250/313]    eta: 0:00:00  time: 0.0072  data_time: 0.0038  memory: 644  \n",
      "02/05 22:51:38 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][260/313]    eta: 0:00:00  time: 0.0073  data_time: 0.0039  memory: 644  \n",
      "02/05 22:51:38 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][270/313]    eta: 0:00:00  time: 0.0073  data_time: 0.0038  memory: 644  \n",
      "02/05 22:51:39 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][280/313]    eta: 0:00:00  time: 0.0072  data_time: 0.0037  memory: 644  \n",
      "02/05 22:51:39 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][290/313]    eta: 0:00:00  time: 0.0071  data_time: 0.0037  memory: 644  \n",
      "02/05 22:51:39 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][300/313]    eta: 0:00:00  time: 0.0072  data_time: 0.0038  memory: 644  \n",
      "02/05 22:51:39 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][310/313]    eta: 0:00:00  time: 0.0071  data_time: 0.0037  memory: 644  \n",
      "02/05 22:51:39 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][313/313]    accuracy: 49.1900  data_time: 0.0037  time: 0.0073\n",
      "02/05 22:51:39 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][  10/1563]  lr: 1.0000e-03  eta: 0:00:30  time: 0.0211  data_time: 0.0071  memory: 693  loss: 1.4631\n",
      "02/05 22:51:39 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][  20/1563]  lr: 1.0000e-03  eta: 0:00:29  time: 0.0209  data_time: 0.0069  memory: 693  loss: 1.5230\n",
      "02/05 22:51:39 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][  30/1563]  lr: 1.0000e-03  eta: 0:00:29  time: 0.0204  data_time: 0.0063  memory: 693  loss: 1.4975\n",
      "02/05 22:51:40 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][  40/1563]  lr: 1.0000e-03  eta: 0:00:29  time: 0.0203  data_time: 0.0062  memory: 693  loss: 1.4192\n",
      "02/05 22:51:40 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][  50/1563]  lr: 1.0000e-03  eta: 0:00:29  time: 0.0200  data_time: 0.0060  memory: 693  loss: 1.4591\n",
      "02/05 22:51:40 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][  60/1563]  lr: 1.0000e-03  eta: 0:00:29  time: 0.0200  data_time: 0.0060  memory: 693  loss: 1.4420\n",
      "02/05 22:51:40 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][  70/1563]  lr: 1.0000e-03  eta: 0:00:29  time: 0.0205  data_time: 0.0064  memory: 693  loss: 1.4859\n",
      "02/05 22:51:40 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][  80/1563]  lr: 1.0000e-03  eta: 0:00:28  time: 0.0201  data_time: 0.0061  memory: 693  loss: 1.5122\n",
      "02/05 22:51:41 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][  90/1563]  lr: 1.0000e-03  eta: 0:00:28  time: 0.0201  data_time: 0.0061  memory: 693  loss: 1.6176\n",
      "02/05 22:51:41 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 100/1563]  lr: 1.0000e-03  eta: 0:00:28  time: 0.0215  data_time: 0.0076  memory: 693  loss: 1.5284\n",
      "02/05 22:51:41 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 110/1563]  lr: 1.0000e-03  eta: 0:00:28  time: 0.0222  data_time: 0.0064  memory: 693  loss: 1.3888\n",
      "02/05 22:51:41 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 120/1563]  lr: 1.0000e-03  eta: 0:00:28  time: 0.0207  data_time: 0.0068  memory: 693  loss: 1.5985\n",
      "02/05 22:51:41 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 130/1563]  lr: 1.0000e-03  eta: 0:00:27  time: 0.0204  data_time: 0.0064  memory: 693  loss: 1.4930\n",
      "02/05 22:51:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 140/1563]  lr: 1.0000e-03  eta: 0:00:27  time: 0.0203  data_time: 0.0063  memory: 693  loss: 1.5507\n",
      "02/05 22:51:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 150/1563]  lr: 1.0000e-03  eta: 0:00:27  time: 0.0201  data_time: 0.0061  memory: 693  loss: 1.5369\n",
      "02/05 22:51:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 160/1563]  lr: 1.0000e-03  eta: 0:00:27  time: 0.0201  data_time: 0.0061  memory: 693  loss: 1.4918\n",
      "02/05 22:51:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 170/1563]  lr: 1.0000e-03  eta: 0:00:27  time: 0.0201  data_time: 0.0062  memory: 693  loss: 1.4742\n",
      "02/05 22:51:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 180/1563]  lr: 1.0000e-03  eta: 0:00:26  time: 0.0200  data_time: 0.0060  memory: 693  loss: 1.4615\n",
      "02/05 22:51:43 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 190/1563]  lr: 1.0000e-03  eta: 0:00:26  time: 0.0199  data_time: 0.0059  memory: 693  loss: 1.3329\n",
      "02/05 22:51:43 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 200/1563]  lr: 1.0000e-03  eta: 0:00:26  time: 0.0198  data_time: 0.0058  memory: 693  loss: 1.5079\n",
      "02/05 22:51:43 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 210/1563]  lr: 1.0000e-03  eta: 0:00:26  time: 0.0199  data_time: 0.0059  memory: 693  loss: 1.5865\n",
      "02/05 22:51:43 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 220/1563]  lr: 1.0000e-03  eta: 0:00:26  time: 0.0199  data_time: 0.0059  memory: 693  loss: 1.5219\n",
      "02/05 22:51:43 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 230/1563]  lr: 1.0000e-03  eta: 0:00:26  time: 0.0197  data_time: 0.0058  memory: 693  loss: 1.4665\n",
      "02/05 22:51:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 240/1563]  lr: 1.0000e-03  eta: 0:00:25  time: 0.0197  data_time: 0.0057  memory: 693  loss: 1.5353\n",
      "02/05 22:51:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 250/1563]  lr: 1.0000e-03  eta: 0:00:25  time: 0.0197  data_time: 0.0057  memory: 693  loss: 1.4881\n",
      "02/05 22:51:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 260/1563]  lr: 1.0000e-03  eta: 0:00:25  time: 0.0198  data_time: 0.0058  memory: 693  loss: 1.6457\n",
      "02/05 22:51:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 270/1563]  lr: 1.0000e-03  eta: 0:00:25  time: 0.0198  data_time: 0.0058  memory: 693  loss: 1.4355\n",
      "02/05 22:51:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 280/1563]  lr: 1.0000e-03  eta: 0:00:25  time: 0.0200  data_time: 0.0060  memory: 693  loss: 1.4806\n",
      "02/05 22:51:45 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 290/1563]  lr: 1.0000e-03  eta: 0:00:24  time: 0.0212  data_time: 0.0071  memory: 693  loss: 1.5765\n",
      "02/05 22:51:45 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 300/1563]  lr: 1.0000e-03  eta: 0:00:24  time: 0.0215  data_time: 0.0074  memory: 693  loss: 1.4710\n",
      "02/05 22:51:45 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 310/1563]  lr: 1.0000e-03  eta: 0:00:24  time: 0.0211  data_time: 0.0069  memory: 693  loss: 1.5729\n",
      "02/05 22:51:45 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 320/1563]  lr: 1.0000e-03  eta: 0:00:24  time: 0.0204  data_time: 0.0064  memory: 693  loss: 1.4905\n",
      "02/05 22:51:46 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 330/1563]  lr: 1.0000e-03  eta: 0:00:24  time: 0.0203  data_time: 0.0064  memory: 693  loss: 1.4003\n",
      "02/05 22:51:46 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 340/1563]  lr: 1.0000e-03  eta: 0:00:23  time: 0.0202  data_time: 0.0062  memory: 693  loss: 1.4145\n",
      "02/05 22:51:46 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 350/1563]  lr: 1.0000e-03  eta: 0:00:23  time: 0.0213  data_time: 0.0073  memory: 693  loss: 1.3837\n",
      "02/05 22:51:46 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 360/1563]  lr: 1.0000e-03  eta: 0:00:23  time: 0.0203  data_time: 0.0063  memory: 693  loss: 1.4954\n",
      "02/05 22:51:46 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 370/1563]  lr: 1.0000e-03  eta: 0:00:23  time: 0.0207  data_time: 0.0067  memory: 693  loss: 1.5031\n",
      "02/05 22:51:47 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 380/1563]  lr: 1.0000e-03  eta: 0:00:23  time: 0.0200  data_time: 0.0061  memory: 693  loss: 1.4692\n",
      "02/05 22:51:47 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 390/1563]  lr: 1.0000e-03  eta: 0:00:22  time: 0.0200  data_time: 0.0060  memory: 693  loss: 1.3819\n",
      "02/05 22:51:47 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 400/1563]  lr: 1.0000e-03  eta: 0:00:22  time: 0.0200  data_time: 0.0060  memory: 693  loss: 1.4394\n",
      "02/05 22:51:47 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 410/1563]  lr: 1.0000e-03  eta: 0:00:22  time: 0.0203  data_time: 0.0063  memory: 693  loss: 1.4520\n",
      "02/05 22:51:47 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 420/1563]  lr: 1.0000e-03  eta: 0:00:22  time: 0.0204  data_time: 0.0064  memory: 693  loss: 1.5031\n",
      "02/05 22:51:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 430/1563]  lr: 1.0000e-03  eta: 0:00:22  time: 0.0200  data_time: 0.0061  memory: 693  loss: 1.3864\n",
      "02/05 22:51:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: 20240205_225106\n",
      "02/05 22:51:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 440/1563]  lr: 1.0000e-03  eta: 0:00:22  time: 0.0204  data_time: 0.0063  memory: 693  loss: 1.4806\n",
      "02/05 22:51:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 450/1563]  lr: 1.0000e-03  eta: 0:00:21  time: 0.0200  data_time: 0.0060  memory: 693  loss: 1.4310\n",
      "02/05 22:51:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 460/1563]  lr: 1.0000e-03  eta: 0:00:21  time: 0.0201  data_time: 0.0061  memory: 693  loss: 1.5516\n",
      "02/05 22:51:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 470/1563]  lr: 1.0000e-03  eta: 0:00:21  time: 0.0201  data_time: 0.0061  memory: 693  loss: 1.4376\n",
      "02/05 22:51:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 480/1563]  lr: 1.0000e-03  eta: 0:00:21  time: 0.0200  data_time: 0.0061  memory: 693  loss: 1.3634\n",
      "02/05 22:51:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 490/1563]  lr: 1.0000e-03  eta: 0:00:21  time: 0.0201  data_time: 0.0061  memory: 693  loss: 1.3098\n",
      "02/05 22:51:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 500/1563]  lr: 1.0000e-03  eta: 0:00:20  time: 0.0200  data_time: 0.0060  memory: 693  loss: 1.4370\n",
      "02/05 22:51:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 510/1563]  lr: 1.0000e-03  eta: 0:00:20  time: 0.0206  data_time: 0.0067  memory: 693  loss: 1.4300\n",
      "02/05 22:51:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 520/1563]  lr: 1.0000e-03  eta: 0:00:20  time: 0.0199  data_time: 0.0060  memory: 693  loss: 1.4450\n",
      "02/05 22:51:50 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 530/1563]  lr: 1.0000e-03  eta: 0:00:20  time: 0.0202  data_time: 0.0062  memory: 693  loss: 1.4453\n",
      "02/05 22:51:50 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 540/1563]  lr: 1.0000e-03  eta: 0:00:20  time: 0.0204  data_time: 0.0063  memory: 693  loss: 1.5207\n",
      "02/05 22:51:50 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 550/1563]  lr: 1.0000e-03  eta: 0:00:19  time: 0.0209  data_time: 0.0068  memory: 693  loss: 1.4708\n",
      "02/05 22:51:50 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 560/1563]  lr: 1.0000e-03  eta: 0:00:19  time: 0.0203  data_time: 0.0063  memory: 693  loss: 1.4750\n",
      "02/05 22:51:50 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 570/1563]  lr: 1.0000e-03  eta: 0:00:19  time: 0.0213  data_time: 0.0070  memory: 693  loss: 1.3824\n",
      "02/05 22:51:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 580/1563]  lr: 1.0000e-03  eta: 0:00:19  time: 0.0210  data_time: 0.0069  memory: 693  loss: 1.4406\n",
      "02/05 22:51:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 590/1563]  lr: 1.0000e-03  eta: 0:00:19  time: 0.0205  data_time: 0.0065  memory: 693  loss: 1.5989\n",
      "02/05 22:51:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 600/1563]  lr: 1.0000e-03  eta: 0:00:18  time: 0.0204  data_time: 0.0064  memory: 693  loss: 1.4200\n",
      "02/05 22:51:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 610/1563]  lr: 1.0000e-03  eta: 0:00:18  time: 0.0211  data_time: 0.0067  memory: 693  loss: 1.4905\n",
      "02/05 22:51:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 620/1563]  lr: 1.0000e-03  eta: 0:00:18  time: 0.0202  data_time: 0.0062  memory: 693  loss: 1.4773\n",
      "02/05 22:51:52 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 630/1563]  lr: 1.0000e-03  eta: 0:00:18  time: 0.0201  data_time: 0.0061  memory: 693  loss: 1.5689\n",
      "02/05 22:51:52 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 640/1563]  lr: 1.0000e-03  eta: 0:00:18  time: 0.0210  data_time: 0.0069  memory: 693  loss: 1.5298\n",
      "02/05 22:51:52 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 650/1563]  lr: 1.0000e-03  eta: 0:00:17  time: 0.0202  data_time: 0.0062  memory: 693  loss: 1.3646\n",
      "02/05 22:51:52 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 660/1563]  lr: 1.0000e-03  eta: 0:00:17  time: 0.0208  data_time: 0.0067  memory: 693  loss: 1.3830\n",
      "02/05 22:51:52 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 670/1563]  lr: 1.0000e-03  eta: 0:00:17  time: 0.0203  data_time: 0.0063  memory: 693  loss: 1.4235\n",
      "02/05 22:51:53 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 680/1563]  lr: 1.0000e-03  eta: 0:00:17  time: 0.0202  data_time: 0.0062  memory: 693  loss: 1.3776\n",
      "02/05 22:51:53 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 690/1563]  lr: 1.0000e-03  eta: 0:00:17  time: 0.0206  data_time: 0.0066  memory: 693  loss: 1.4731\n",
      "02/05 22:51:53 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 700/1563]  lr: 1.0000e-03  eta: 0:00:16  time: 0.0201  data_time: 0.0061  memory: 693  loss: 1.4003\n",
      "02/05 22:51:53 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 710/1563]  lr: 1.0000e-03  eta: 0:00:16  time: 0.0207  data_time: 0.0065  memory: 693  loss: 1.4086\n",
      "02/05 22:51:53 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 720/1563]  lr: 1.0000e-03  eta: 0:00:16  time: 0.0201  data_time: 0.0062  memory: 693  loss: 1.4321\n",
      "02/05 22:51:54 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 730/1563]  lr: 1.0000e-03  eta: 0:00:16  time: 0.0204  data_time: 0.0063  memory: 693  loss: 1.4808\n",
      "02/05 22:51:54 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 740/1563]  lr: 1.0000e-03  eta: 0:00:16  time: 0.0201  data_time: 0.0062  memory: 693  loss: 1.5116\n",
      "02/05 22:51:54 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 750/1563]  lr: 1.0000e-03  eta: 0:00:16  time: 0.0207  data_time: 0.0066  memory: 693  loss: 1.4395\n",
      "02/05 22:51:54 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 760/1563]  lr: 1.0000e-03  eta: 0:00:15  time: 0.0209  data_time: 0.0068  memory: 693  loss: 1.5673\n",
      "02/05 22:51:54 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 770/1563]  lr: 1.0000e-03  eta: 0:00:15  time: 0.0204  data_time: 0.0065  memory: 693  loss: 1.4200\n",
      "02/05 22:51:55 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 780/1563]  lr: 1.0000e-03  eta: 0:00:15  time: 0.0200  data_time: 0.0061  memory: 693  loss: 1.4943\n",
      "02/05 22:51:55 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 790/1563]  lr: 1.0000e-03  eta: 0:00:15  time: 0.0205  data_time: 0.0065  memory: 693  loss: 1.4495\n",
      "02/05 22:51:55 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 800/1563]  lr: 1.0000e-03  eta: 0:00:15  time: 0.0199  data_time: 0.0059  memory: 693  loss: 1.2719\n",
      "02/05 22:51:55 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 810/1563]  lr: 1.0000e-03  eta: 0:00:14  time: 0.0208  data_time: 0.0066  memory: 693  loss: 1.3263\n",
      "02/05 22:51:56 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 820/1563]  lr: 1.0000e-03  eta: 0:00:14  time: 0.0203  data_time: 0.0064  memory: 693  loss: 1.4153\n",
      "02/05 22:51:56 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 830/1563]  lr: 1.0000e-03  eta: 0:00:14  time: 0.0210  data_time: 0.0068  memory: 693  loss: 1.3173\n",
      "02/05 22:51:56 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 840/1563]  lr: 1.0000e-03  eta: 0:00:14  time: 0.0202  data_time: 0.0063  memory: 693  loss: 1.4057\n",
      "02/05 22:51:56 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 850/1563]  lr: 1.0000e-03  eta: 0:00:14  time: 0.0210  data_time: 0.0069  memory: 693  loss: 1.4907\n",
      "02/05 22:51:56 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 860/1563]  lr: 1.0000e-03  eta: 0:00:13  time: 0.0204  data_time: 0.0064  memory: 693  loss: 1.4728\n",
      "02/05 22:51:57 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 870/1563]  lr: 1.0000e-03  eta: 0:00:13  time: 0.0205  data_time: 0.0064  memory: 693  loss: 1.3721\n",
      "02/05 22:51:57 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 880/1563]  lr: 1.0000e-03  eta: 0:00:13  time: 0.0202  data_time: 0.0063  memory: 693  loss: 1.4724\n",
      "02/05 22:51:57 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 890/1563]  lr: 1.0000e-03  eta: 0:00:13  time: 0.0205  data_time: 0.0064  memory: 693  loss: 1.3637\n",
      "02/05 22:51:57 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 900/1563]  lr: 1.0000e-03  eta: 0:00:13  time: 0.0202  data_time: 0.0062  memory: 693  loss: 1.4962\n",
      "02/05 22:51:57 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 910/1563]  lr: 1.0000e-03  eta: 0:00:12  time: 0.0201  data_time: 0.0062  memory: 693  loss: 1.5108\n",
      "02/05 22:51:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 920/1563]  lr: 1.0000e-03  eta: 0:00:12  time: 0.0209  data_time: 0.0071  memory: 693  loss: 1.3963\n",
      "02/05 22:51:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 930/1563]  lr: 1.0000e-03  eta: 0:00:12  time: 0.0200  data_time: 0.0061  memory: 693  loss: 1.4663\n",
      "02/05 22:51:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 940/1563]  lr: 1.0000e-03  eta: 0:00:12  time: 0.0200  data_time: 0.0061  memory: 693  loss: 1.4100\n",
      "02/05 22:51:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 950/1563]  lr: 1.0000e-03  eta: 0:00:12  time: 0.0201  data_time: 0.0062  memory: 693  loss: 1.5087\n",
      "02/05 22:51:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 960/1563]  lr: 1.0000e-03  eta: 0:00:11  time: 0.0201  data_time: 0.0061  memory: 693  loss: 1.4327\n",
      "02/05 22:51:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 970/1563]  lr: 1.0000e-03  eta: 0:00:11  time: 0.0201  data_time: 0.0061  memory: 693  loss: 1.4490\n",
      "02/05 22:51:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 980/1563]  lr: 1.0000e-03  eta: 0:00:11  time: 0.0200  data_time: 0.0061  memory: 693  loss: 1.4712\n",
      "02/05 22:51:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][ 990/1563]  lr: 1.0000e-03  eta: 0:00:11  time: 0.0199  data_time: 0.0060  memory: 693  loss: 1.3859\n",
      "02/05 22:51:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1000/1563]  lr: 1.0000e-03  eta: 0:00:11  time: 0.0198  data_time: 0.0059  memory: 693  loss: 1.4141\n",
      "02/05 22:51:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1010/1563]  lr: 1.0000e-03  eta: 0:00:10  time: 0.0199  data_time: 0.0060  memory: 693  loss: 1.4762\n",
      "02/05 22:52:00 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1020/1563]  lr: 1.0000e-03  eta: 0:00:10  time: 0.0198  data_time: 0.0059  memory: 693  loss: 1.3983\n",
      "02/05 22:52:00 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1030/1563]  lr: 1.0000e-03  eta: 0:00:10  time: 0.0198  data_time: 0.0059  memory: 693  loss: 1.4552\n",
      "02/05 22:52:00 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1040/1563]  lr: 1.0000e-03  eta: 0:00:10  time: 0.0198  data_time: 0.0059  memory: 693  loss: 1.3662\n",
      "02/05 22:52:00 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1050/1563]  lr: 1.0000e-03  eta: 0:00:10  time: 0.0198  data_time: 0.0059  memory: 693  loss: 1.4220\n",
      "02/05 22:52:00 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1060/1563]  lr: 1.0000e-03  eta: 0:00:09  time: 0.0201  data_time: 0.0059  memory: 693  loss: 1.4265\n",
      "02/05 22:52:01 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1070/1563]  lr: 1.0000e-03  eta: 0:00:09  time: 0.0202  data_time: 0.0063  memory: 693  loss: 1.4554\n",
      "02/05 22:52:01 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1080/1563]  lr: 1.0000e-03  eta: 0:00:09  time: 0.0200  data_time: 0.0061  memory: 693  loss: 1.3516\n",
      "02/05 22:52:01 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1090/1563]  lr: 1.0000e-03  eta: 0:00:09  time: 0.0198  data_time: 0.0059  memory: 693  loss: 1.3684\n",
      "02/05 22:52:01 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1100/1563]  lr: 1.0000e-03  eta: 0:00:09  time: 0.0199  data_time: 0.0059  memory: 693  loss: 1.3823\n",
      "02/05 22:52:01 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1110/1563]  lr: 1.0000e-03  eta: 0:00:08  time: 0.0198  data_time: 0.0059  memory: 693  loss: 1.2385\n",
      "02/05 22:52:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1120/1563]  lr: 1.0000e-03  eta: 0:00:08  time: 0.0196  data_time: 0.0057  memory: 693  loss: 1.3986\n",
      "02/05 22:52:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1130/1563]  lr: 1.0000e-03  eta: 0:00:08  time: 0.0196  data_time: 0.0057  memory: 693  loss: 1.4771\n",
      "02/05 22:52:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1140/1563]  lr: 1.0000e-03  eta: 0:00:08  time: 0.0196  data_time: 0.0057  memory: 693  loss: 1.4441\n",
      "02/05 22:52:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1150/1563]  lr: 1.0000e-03  eta: 0:00:08  time: 0.0197  data_time: 0.0058  memory: 693  loss: 1.5211\n",
      "02/05 22:52:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1160/1563]  lr: 1.0000e-03  eta: 0:00:07  time: 0.0199  data_time: 0.0059  memory: 693  loss: 1.3888\n",
      "02/05 22:52:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1170/1563]  lr: 1.0000e-03  eta: 0:00:07  time: 0.0200  data_time: 0.0060  memory: 693  loss: 1.2175\n",
      "02/05 22:52:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1180/1563]  lr: 1.0000e-03  eta: 0:00:07  time: 0.0199  data_time: 0.0059  memory: 693  loss: 1.3443\n",
      "02/05 22:52:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1190/1563]  lr: 1.0000e-03  eta: 0:00:07  time: 0.0198  data_time: 0.0059  memory: 693  loss: 1.3526\n",
      "02/05 22:52:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1200/1563]  lr: 1.0000e-03  eta: 0:00:07  time: 0.0198  data_time: 0.0058  memory: 693  loss: 1.3441\n",
      "02/05 22:52:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1210/1563]  lr: 1.0000e-03  eta: 0:00:06  time: 0.0198  data_time: 0.0060  memory: 693  loss: 1.3850\n",
      "02/05 22:52:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1220/1563]  lr: 1.0000e-03  eta: 0:00:06  time: 0.0206  data_time: 0.0066  memory: 693  loss: 1.4115\n",
      "02/05 22:52:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1230/1563]  lr: 1.0000e-03  eta: 0:00:06  time: 0.0198  data_time: 0.0058  memory: 693  loss: 1.4661\n",
      "02/05 22:52:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1240/1563]  lr: 1.0000e-03  eta: 0:00:06  time: 0.0198  data_time: 0.0059  memory: 693  loss: 1.4403\n",
      "02/05 22:52:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1250/1563]  lr: 1.0000e-03  eta: 0:00:06  time: 0.0198  data_time: 0.0059  memory: 693  loss: 1.4486\n",
      "02/05 22:52:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1260/1563]  lr: 1.0000e-03  eta: 0:00:05  time: 0.0196  data_time: 0.0057  memory: 693  loss: 1.3018\n",
      "02/05 22:52:05 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1270/1563]  lr: 1.0000e-03  eta: 0:00:05  time: 0.0197  data_time: 0.0058  memory: 693  loss: 1.4652\n",
      "02/05 22:52:05 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1280/1563]  lr: 1.0000e-03  eta: 0:00:05  time: 0.0214  data_time: 0.0071  memory: 693  loss: 1.3766\n",
      "02/05 22:52:05 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1290/1563]  lr: 1.0000e-03  eta: 0:00:05  time: 0.0207  data_time: 0.0064  memory: 693  loss: 1.2691\n",
      "02/05 22:52:05 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1300/1563]  lr: 1.0000e-03  eta: 0:00:05  time: 0.0207  data_time: 0.0067  memory: 693  loss: 1.3203\n",
      "02/05 22:52:05 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1310/1563]  lr: 1.0000e-03  eta: 0:00:05  time: 0.0202  data_time: 0.0063  memory: 693  loss: 1.2752\n",
      "02/05 22:52:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1320/1563]  lr: 1.0000e-03  eta: 0:00:04  time: 0.0201  data_time: 0.0063  memory: 693  loss: 1.2805\n",
      "02/05 22:52:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1330/1563]  lr: 1.0000e-03  eta: 0:00:04  time: 0.0209  data_time: 0.0070  memory: 693  loss: 1.3450\n",
      "02/05 22:52:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1340/1563]  lr: 1.0000e-03  eta: 0:00:04  time: 0.0199  data_time: 0.0060  memory: 693  loss: 1.2505\n",
      "02/05 22:52:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1350/1563]  lr: 1.0000e-03  eta: 0:00:04  time: 0.0199  data_time: 0.0060  memory: 693  loss: 1.3992\n",
      "02/05 22:52:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1360/1563]  lr: 1.0000e-03  eta: 0:00:04  time: 0.0197  data_time: 0.0059  memory: 693  loss: 1.4785\n",
      "02/05 22:52:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1370/1563]  lr: 1.0000e-03  eta: 0:00:03  time: 0.0199  data_time: 0.0060  memory: 693  loss: 1.2637\n",
      "02/05 22:52:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1380/1563]  lr: 1.0000e-03  eta: 0:00:03  time: 0.0198  data_time: 0.0059  memory: 693  loss: 1.3168\n",
      "02/05 22:52:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1390/1563]  lr: 1.0000e-03  eta: 0:00:03  time: 0.0199  data_time: 0.0060  memory: 693  loss: 1.3492\n",
      "02/05 22:52:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1400/1563]  lr: 1.0000e-03  eta: 0:00:03  time: 0.0199  data_time: 0.0060  memory: 693  loss: 1.3764\n",
      "02/05 22:52:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1410/1563]  lr: 1.0000e-03  eta: 0:00:03  time: 0.0198  data_time: 0.0059  memory: 693  loss: 1.3612\n",
      "02/05 22:52:08 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1420/1563]  lr: 1.0000e-03  eta: 0:00:02  time: 0.0197  data_time: 0.0058  memory: 693  loss: 1.4217\n",
      "02/05 22:52:08 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1430/1563]  lr: 1.0000e-03  eta: 0:00:02  time: 0.0198  data_time: 0.0059  memory: 693  loss: 1.4570\n",
      "02/05 22:52:08 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: 20240205_225106\n",
      "02/05 22:52:08 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1440/1563]  lr: 1.0000e-03  eta: 0:00:02  time: 0.0200  data_time: 0.0061  memory: 693  loss: 1.4447\n",
      "02/05 22:52:08 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1450/1563]  lr: 1.0000e-03  eta: 0:00:02  time: 0.0199  data_time: 0.0060  memory: 693  loss: 1.4125\n",
      "02/05 22:52:08 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1460/1563]  lr: 1.0000e-03  eta: 0:00:02  time: 0.0198  data_time: 0.0059  memory: 693  loss: 1.3064\n",
      "02/05 22:52:09 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1470/1563]  lr: 1.0000e-03  eta: 0:00:01  time: 0.0199  data_time: 0.0060  memory: 693  loss: 1.4223\n",
      "02/05 22:52:09 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1480/1563]  lr: 1.0000e-03  eta: 0:00:01  time: 0.0198  data_time: 0.0059  memory: 693  loss: 1.3330\n",
      "02/05 22:52:09 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1490/1563]  lr: 1.0000e-03  eta: 0:00:01  time: 0.0197  data_time: 0.0059  memory: 693  loss: 1.3582\n",
      "02/05 22:52:09 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1500/1563]  lr: 1.0000e-03  eta: 0:00:01  time: 0.0198  data_time: 0.0059  memory: 693  loss: 1.4222\n",
      "02/05 22:52:09 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1510/1563]  lr: 1.0000e-03  eta: 0:00:01  time: 0.0198  data_time: 0.0059  memory: 693  loss: 1.3537\n",
      "02/05 22:52:10 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1520/1563]  lr: 1.0000e-03  eta: 0:00:00  time: 0.0198  data_time: 0.0059  memory: 693  loss: 1.3167\n",
      "02/05 22:52:10 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1530/1563]  lr: 1.0000e-03  eta: 0:00:00  time: 0.0198  data_time: 0.0059  memory: 693  loss: 1.3404\n",
      "02/05 22:52:10 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1540/1563]  lr: 1.0000e-03  eta: 0:00:00  time: 0.0199  data_time: 0.0060  memory: 693  loss: 1.4121\n",
      "02/05 22:52:10 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1550/1563]  lr: 1.0000e-03  eta: 0:00:00  time: 0.0198  data_time: 0.0059  memory: 693  loss: 1.3706\n",
      "02/05 22:52:10 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [2][1560/1563]  lr: 1.0000e-03  eta: 0:00:00  time: 0.0199  data_time: 0.0061  memory: 693  loss: 1.4051\n",
      "02/05 22:52:10 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: 20240205_225106\n",
      "02/05 22:52:10 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 2 epochs\n",
      "02/05 22:52:11 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][ 10/313]    eta: 0:00:02  time: 0.0067  data_time: 0.0034  memory: 693  \n",
      "02/05 22:52:11 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][ 20/313]    eta: 0:00:01  time: 0.0069  data_time: 0.0035  memory: 644  \n",
      "02/05 22:52:11 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][ 30/313]    eta: 0:00:01  time: 0.0070  data_time: 0.0036  memory: 644  \n",
      "02/05 22:52:11 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][ 40/313]    eta: 0:00:01  time: 0.0071  data_time: 0.0037  memory: 644  \n",
      "02/05 22:52:11 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][ 50/313]    eta: 0:00:01  time: 0.0070  data_time: 0.0036  memory: 644  \n",
      "02/05 22:52:11 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][ 60/313]    eta: 0:00:01  time: 0.0070  data_time: 0.0036  memory: 644  \n",
      "02/05 22:52:11 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][ 70/313]    eta: 0:00:01  time: 0.0070  data_time: 0.0036  memory: 644  \n",
      "02/05 22:52:11 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][ 80/313]    eta: 0:00:01  time: 0.0071  data_time: 0.0037  memory: 644  \n",
      "02/05 22:52:11 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][ 90/313]    eta: 0:00:01  time: 0.0071  data_time: 0.0036  memory: 644  \n",
      "02/05 22:52:11 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][100/313]    eta: 0:00:01  time: 0.0072  data_time: 0.0038  memory: 644  \n",
      "02/05 22:52:11 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][110/313]    eta: 0:00:01  time: 0.0087  data_time: 0.0049  memory: 644  \n",
      "02/05 22:52:11 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][120/313]    eta: 0:00:01  time: 0.0074  data_time: 0.0040  memory: 644  \n",
      "02/05 22:52:12 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][130/313]    eta: 0:00:01  time: 0.0076  data_time: 0.0041  memory: 644  \n",
      "02/05 22:52:12 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][140/313]    eta: 0:00:01  time: 0.0075  data_time: 0.0040  memory: 644  \n",
      "02/05 22:52:12 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][150/313]    eta: 0:00:01  time: 0.0075  data_time: 0.0040  memory: 644  \n",
      "02/05 22:52:12 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][160/313]    eta: 0:00:01  time: 0.0074  data_time: 0.0039  memory: 644  \n",
      "02/05 22:52:12 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][170/313]    eta: 0:00:01  time: 0.0087  data_time: 0.0049  memory: 644  \n",
      "02/05 22:52:12 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][180/313]    eta: 0:00:00  time: 0.0077  data_time: 0.0042  memory: 644  \n",
      "02/05 22:52:12 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][190/313]    eta: 0:00:00  time: 0.0077  data_time: 0.0042  memory: 644  \n",
      "02/05 22:52:12 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][200/313]    eta: 0:00:00  time: 0.0079  data_time: 0.0043  memory: 644  \n",
      "02/05 22:52:12 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][210/313]    eta: 0:00:00  time: 0.0076  data_time: 0.0041  memory: 644  \n",
      "02/05 22:52:12 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][220/313]    eta: 0:00:00  time: 0.0075  data_time: 0.0041  memory: 644  \n",
      "02/05 22:52:12 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][230/313]    eta: 0:00:00  time: 0.0075  data_time: 0.0040  memory: 644  \n",
      "02/05 22:52:12 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][240/313]    eta: 0:00:00  time: 0.0073  data_time: 0.0038  memory: 644  \n",
      "02/05 22:52:12 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][250/313]    eta: 0:00:00  time: 0.0074  data_time: 0.0039  memory: 644  \n",
      "02/05 22:52:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][260/313]    eta: 0:00:00  time: 0.0074  data_time: 0.0040  memory: 644  \n",
      "02/05 22:52:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][270/313]    eta: 0:00:00  time: 0.0073  data_time: 0.0039  memory: 644  \n",
      "02/05 22:52:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][280/313]    eta: 0:00:00  time: 0.0074  data_time: 0.0039  memory: 644  \n",
      "02/05 22:52:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][290/313]    eta: 0:00:00  time: 0.0073  data_time: 0.0039  memory: 644  \n",
      "02/05 22:52:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][300/313]    eta: 0:00:00  time: 0.0074  data_time: 0.0039  memory: 644  \n",
      "02/05 22:52:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][310/313]    eta: 0:00:00  time: 0.0074  data_time: 0.0040  memory: 644  \n",
      "02/05 22:52:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [2][313/313]    accuracy: 54.8000  data_time: 0.0039  time: 0.0074\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ModelWrapper(\n",
       "  (data_preprocessor): BaseDataPreprocessor()\n",
       "  (model): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.optim import SGD\n",
    "from mmengine.runner import Runner\n",
    "\n",
    "\n",
    "runner = Runner(\n",
    "    # the model used for training and validation.\n",
    "    # Needs to meet specific interface requirements\n",
    "    #model=MMViT_B16(vit_b_16),\n",
    "    model=model,\n",
    "    # working directory which saves training logs and weight files\n",
    "    work_dir=work_dir,\n",
    "    # train dataloader needs to meet the PyTorch data loader protocol\n",
    "    train_dataloader=train_dataloader,\n",
    "    # optimize wrapper for optimization with additional features like\n",
    "    # AMP, gradtient accumulation, etc\n",
    "    optim_wrapper=dict(optimizer=dict(type=SGD, lr=0.001, momentum=0.9)),\n",
    "    # trainging coinfs for specifying training epoches, verification intervals, etc\n",
    "    train_cfg=dict(by_epoch=True, max_epochs=2, val_interval=1),\n",
    "    # validation dataloaer also needs to meet the PyTorch data loader protocol\n",
    "    val_dataloader=val_dataloader,\n",
    "    # validation configs for specifying additional parameters required for validation\n",
    "    val_cfg=dict(),\n",
    "    # validation evaluator. The default one is used here\n",
    "    val_evaluator=dict(type=Accuracy),\n",
    ")\n",
    "\n",
    "runner.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fin!\n"
     ]
    }
   ],
   "source": [
    "print('Fin!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
