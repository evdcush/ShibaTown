{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MMEngine Quickstart\n",
    "Build a complete and configurable pipeline for both training and validation with a ~~ResNet-50~~ ConvNeXt model on CIFAR-10.\n",
    "https://mmengine.readthedocs.io/en/latest/get_started/15_minutes.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Model\n",
    "Models in MMEngine should inherit from **`model.BaseModel`**, a simple model interface that can optionally contain a data preprocessor as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "from mmengine.model import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MMConvNeXtTiny(BaseModel):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = torchvision.models.convnext_tiny()\n",
    "    \n",
    "    def forward(self, images, targets, mode):\n",
    "        \"\"\"\n",
    "        mode (str): determines the model's forward path.\n",
    "            TRAINING: mode='loss', returns a dict containing the key \"loss\".\n",
    "            VALIDATION: mode='predict', returns tuple (preds, targets).\n",
    "            mode='tensor', special case\n",
    "        \"\"\"\n",
    "        x = self.model(images)\n",
    "        if mode == 'loss':\n",
    "            return {'loss': F.cross_entropy(x, targets)}\n",
    "        elif mode == 'predict':\n",
    "            return x, targets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Setup the pathing.\n",
    "HOME_ROOT = os.getenv('HOME')\n",
    "DATA_ROOT = os.path.join(HOME_ROOT, '.Data')\n",
    "data_path = DATA_ROOT\n",
    "\n",
    "# Build the dataloaders.\n",
    "norm_cfg = dict(mean=[0.491, 0.482, 0.447], std=[0.202, 0.199, 0.201])\n",
    "train_dataloader = DataLoader(batch_size=32,\n",
    "                              shuffle=True,\n",
    "                              dataset=torchvision.datasets.CIFAR10(\n",
    "                                  #'data/cifar10',\n",
    "                                  data_path + '/cifar10',\n",
    "                                  train=True,\n",
    "                                  download=True,\n",
    "                                  transform=transforms.Compose([\n",
    "                                      transforms.RandomCrop(32, padding=4),\n",
    "                                      transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize(**norm_cfg)\n",
    "                                  ])))\n",
    "\n",
    "val_dataloader = DataLoader(batch_size=32,\n",
    "                            shuffle=False,\n",
    "                            dataset=torchvision.datasets.CIFAR10(\n",
    "                                data_path + '/cifar10',\n",
    "                                train=False,\n",
    "                                download=True,\n",
    "                                transform=transforms.Compose([\n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize(**norm_cfg)\n",
    "                                ])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "source": [
    "## Evaluation `Metric`\n",
    "```\n",
    "Init signature:\n",
    "BaseMetric(\n",
    "    collect_device: str = 'cpu',\n",
    "    prefix: Optional[str] = None,\n",
    "    collect_dir: Optional[str] = None,\n",
    ") -> None\n",
    "Docstring:     \n",
    "Base class for a metric.\n",
    "\n",
    "The metric first processes each batch of data_samples and predictions,\n",
    "and appends the processed results to the results list. Then it\n",
    "collects all results together from all ranks if distributed training\n",
    "is used. Finally, it computes the metrics of the entire dataset.\n",
    "\n",
    "A subclass of class:`BaseMetric` should assign a meaningful value to the\n",
    "class attribute `default_prefix`. See the argument `prefix` for details.\n",
    "\n",
    "Args:\n",
    "    collect_device (str): Device name used for collecting results from\n",
    "        different ranks during distributed training. Must be 'cpu' or\n",
    "        'gpu'. Defaults to 'cpu'.\n",
    "    prefix (str, optional): The prefix that will be added in the metric\n",
    "        names to disambiguate homonymous metrics of different evaluators.\n",
    "        If prefix is not provided in the argument, self.default_prefix\n",
    "        will be used instead. Default: None\n",
    "    collect_dir: (str, optional): Synchronize directory for collecting data\n",
    "        from different ranks. This argument should only be configured when\n",
    "        ``collect_device`` is 'cpu'. Defaults to None.\n",
    "        `New in version 0.7.3.`\n",
    "File:           ~/.pyenv/versions/3.11.2/envs/3112/lib/python3.11/site-packages/mmengine/evaluator/metric.py\n",
    "Type:           ABCMeta\n",
    "Subclasses:     DumpResults\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmengine.evaluator import BaseMetric\n",
    "\n",
    "class Accuracy(BaseMetric):\n",
    "    def process(self, data_batch, data_samples):\n",
    "        score, gt = data_samples\n",
    "        self.results.append({\n",
    "            'batch_size': len(gt),\n",
    "            'correct': (score.argmax(dim=1) == gt).sum().cpu(),\n",
    "        })\n",
    "    \n",
    "    def compute_metrics(self, results):\n",
    "        total_correct = sum(item['correct'] for item in results)\n",
    "        total_size    = sum(item['batch_size'] for item in results)\n",
    "        # Return a dict containing the eval results,\n",
    "        # where the key is the name of the metric.\n",
    "        accuracy = 100 * total_correct / total_size\n",
    "        return dict(accuracy=accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a `Runner` and SEND IT ü§ôüèª"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/11 21:06:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - \n",
      "------------------------------------------------------------\n",
      "System environment:\n",
      "    sys.platform: linux\n",
      "    Python: 3.11.2 (main, Mar  1 2023, 02:01:39) [GCC 10.3.0]\n",
      "    CUDA available: True\n",
      "    numpy_random_seed: 1740429921\n",
      "    GPU 0: NVIDIA GeForce RTX 3090\n",
      "    CUDA_HOME: /usr/local/cuda\n",
      "    NVCC: Cuda compilation tools, release 12.1, V12.1.105\n",
      "    GCC: gcc (Ubuntu 10.3.0-1ubuntu1~20.04) 10.3.0\n",
      "    PyTorch: 2.0.1+cu118\n",
      "    PyTorch compiling details: PyTorch built with:\n",
      "  - GCC 9.3\n",
      "  - C++ Version: 201703\n",
      "  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications\n",
      "  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)\n",
      "  - OpenMP 201511 (a.k.a. OpenMP 4.5)\n",
      "  - LAPACK is enabled (usually provided by MKL)\n",
      "  - NNPACK is enabled\n",
      "  - CPU capability usage: AVX2\n",
      "  - CUDA Runtime 11.8\n",
      "  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90\n",
      "  - CuDNN 8.7\n",
      "  - Magma 2.6.1\n",
      "  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.8, CUDNN_VERSION=8.7.0, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, \n",
      "\n",
      "    TorchVision: 0.15.2+cu118\n",
      "    OpenCV: 4.7.0\n",
      "    MMEngine: 0.7.3\n",
      "\n",
      "Runtime environment:\n",
      "    dist_cfg: {'backend': 'nccl'}\n",
      "    seed: None\n",
      "    Distributed launcher: none\n",
      "    Distributed training: False\n",
      "    GPU number: 1\n",
      "------------------------------------------------------------\n",
      "\n",
      "06/11 21:06:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.\n",
      "06/11 21:06:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "before_train:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(NORMAL      ) DistSamplerSeedHook                \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      "(LOW         ) ParamSchedulerHook                 \n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "after_train:\n",
      "(VERY_LOW    ) CheckpointHook                     \n",
      " -------------------- \n",
      "before_test_epoch:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "before_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_test_iter:\n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_test_epoch:\n",
      "(VERY_HIGH   ) RuntimeInfoHook                    \n",
      "(NORMAL      ) IterTimerHook                      \n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n",
      "after_run:\n",
      "(BELOW_NORMAL) LoggerHook                         \n",
      " -------------------- \n"
     ]
    }
   ],
   "source": [
    "from torch.optim import SGD\n",
    "from mmengine.runner import Runner\n",
    "\n",
    "# Pathing for experiments.\n",
    "EXPERIMENTS_PATH = f\"{HOME_ROOT}/Projects/Experiments/mmsandbox\"\n",
    "if not os.path.exists(EXPERIMENTS_PATH):\n",
    "    os.makedirs(EXPERIMENTS_PATH)\n",
    "\n",
    "runner = Runner(\n",
    "    # The model used for training and validation.\n",
    "    # Needs to meet specific interface requirements.\n",
    "    model=MMConvNeXtTiny(),\n",
    "    # Working directory for training logs and model artifacts.\n",
    "    work_dir=EXPERIMENTS_PATH,\n",
    "    # `train_dataloader` must adhere to PyTorch DataLoader protocol.\n",
    "    train_dataloader=train_dataloader,\n",
    "    # Optimizer wrapper for optimization with additional features\n",
    "    # like AMP, gradient accumulation, etc.\n",
    "    optim_wrapper=dict(\n",
    "        optimizer=dict(type=SGD, lr=0.001, momentum=0.9),\n",
    "    ),\n",
    "    # Training configs for specifying training epoches,\n",
    "    # validation intervals, etc.\n",
    "    train_cfg=dict(\n",
    "        by_epoch=True, max_epochs=1, val_interval=1,\n",
    "    ),\n",
    "    # Validation dataloader also needs to meet the PyTorch\n",
    "    # DataLoader protocol.\n",
    "    val_dataloader=val_dataloader,\n",
    "    # Validation configs for specifying additional parameters\n",
    "    # required for validation.\n",
    "    val_cfg=dict(),\n",
    "    # Validation evaluator. \"We use the default one.\"\n",
    "    val_evaluator=dict(type=Accuracy),    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06/11 21:06:42 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Dataset CIFAR10 has no metainfo. ``dataset_meta`` in visualizer will be None.\n",
      "06/11 21:06:42 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - The prefix is not set in metric class Accuracy.\n",
      "06/11 21:06:42 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - Dataset CIFAR10 has no metainfo. ``dataset_meta`` in evaluator, metric and visualizer will be None.\n",
      "06/11 21:06:42 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - \"FileClient\" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io\n",
      "06/11 21:06:42 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Checkpoints will be saved to /home/evan/Projects/Experiments/mmsandbox.\n",
      "06/11 21:06:43 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][  10/1563]  lr: 1.0000e-03  eta: 0:00:50  time: 0.0326  data_time: 0.0111  memory: 1053  loss: 6.4243\n",
      "06/11 21:06:43 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][  20/1563]  lr: 1.0000e-03  eta: 0:00:49  time: 0.0310  data_time: 0.0113  memory: 1053  loss: 4.5569\n",
      "06/11 21:06:43 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][  30/1563]  lr: 1.0000e-03  eta: 0:00:48  time: 0.0306  data_time: 0.0110  memory: 1053  loss: 3.0619\n",
      "06/11 21:06:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][  40/1563]  lr: 1.0000e-03  eta: 0:00:47  time: 0.0304  data_time: 0.0111  memory: 1053  loss: 2.4836\n",
      "06/11 21:06:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][  50/1563]  lr: 1.0000e-03  eta: 0:00:46  time: 0.0306  data_time: 0.0109  memory: 1053  loss: 2.2884\n",
      "06/11 21:06:44 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][  60/1563]  lr: 1.0000e-03  eta: 0:00:46  time: 0.0311  data_time: 0.0118  memory: 1053  loss: 2.1781\n",
      "06/11 21:06:45 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][  70/1563]  lr: 1.0000e-03  eta: 0:00:46  time: 0.0299  data_time: 0.0108  memory: 1053  loss: 2.0745\n",
      "06/11 21:06:45 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][  80/1563]  lr: 1.0000e-03  eta: 0:00:45  time: 0.0311  data_time: 0.0117  memory: 1053  loss: 2.0400\n",
      "06/11 21:06:45 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][  90/1563]  lr: 1.0000e-03  eta: 0:00:45  time: 0.0307  data_time: 0.0112  memory: 1053  loss: 2.0447\n",
      "06/11 21:06:45 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 100/1563]  lr: 1.0000e-03  eta: 0:00:45  time: 0.0306  data_time: 0.0111  memory: 1053  loss: 2.0662\n",
      "06/11 21:06:46 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 110/1563]  lr: 1.0000e-03  eta: 0:00:44  time: 0.0306  data_time: 0.0118  memory: 1053  loss: 1.9706\n",
      "06/11 21:06:46 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 120/1563]  lr: 1.0000e-03  eta: 0:00:44  time: 0.0302  data_time: 0.0111  memory: 1053  loss: 1.9074\n",
      "06/11 21:06:46 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 130/1563]  lr: 1.0000e-03  eta: 0:00:43  time: 0.0280  data_time: 0.0107  memory: 1053  loss: 2.0402\n",
      "06/11 21:06:47 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 140/1563]  lr: 1.0000e-03  eta: 0:00:43  time: 0.0298  data_time: 0.0110  memory: 1053  loss: 2.0051\n",
      "06/11 21:06:47 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 150/1563]  lr: 1.0000e-03  eta: 0:00:43  time: 0.0295  data_time: 0.0111  memory: 1053  loss: 1.9282\n",
      "06/11 21:06:47 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 160/1563]  lr: 1.0000e-03  eta: 0:00:42  time: 0.0308  data_time: 0.0113  memory: 1053  loss: 2.0274\n",
      "06/11 21:06:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 170/1563]  lr: 1.0000e-03  eta: 0:00:42  time: 0.0305  data_time: 0.0112  memory: 1053  loss: 2.0691\n",
      "06/11 21:06:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 180/1563]  lr: 1.0000e-03  eta: 0:00:41  time: 0.0256  data_time: 0.0096  memory: 1053  loss: 1.9715\n",
      "06/11 21:06:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 190/1563]  lr: 1.0000e-03  eta: 0:00:41  time: 0.0300  data_time: 0.0107  memory: 1053  loss: 1.9792\n",
      "06/11 21:06:48 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 200/1563]  lr: 1.0000e-03  eta: 0:00:41  time: 0.0310  data_time: 0.0115  memory: 1053  loss: 2.0519\n",
      "06/11 21:06:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 210/1563]  lr: 1.0000e-03  eta: 0:00:40  time: 0.0292  data_time: 0.0101  memory: 1053  loss: 2.0117\n",
      "06/11 21:06:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 220/1563]  lr: 1.0000e-03  eta: 0:00:40  time: 0.0254  data_time: 0.0102  memory: 1053  loss: 2.0057\n",
      "06/11 21:06:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 230/1563]  lr: 1.0000e-03  eta: 0:00:39  time: 0.0249  data_time: 0.0104  memory: 1053  loss: 1.9609\n",
      "06/11 21:06:49 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 240/1563]  lr: 1.0000e-03  eta: 0:00:39  time: 0.0260  data_time: 0.0106  memory: 1053  loss: 1.9460\n",
      "06/11 21:06:50 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 250/1563]  lr: 1.0000e-03  eta: 0:00:38  time: 0.0250  data_time: 0.0100  memory: 1053  loss: 1.9594\n",
      "06/11 21:06:50 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 260/1563]  lr: 1.0000e-03  eta: 0:00:38  time: 0.0253  data_time: 0.0104  memory: 1053  loss: 1.9424\n",
      "06/11 21:06:50 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 270/1563]  lr: 1.0000e-03  eta: 0:00:37  time: 0.0294  data_time: 0.0111  memory: 1053  loss: 1.9802\n",
      "06/11 21:06:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 280/1563]  lr: 1.0000e-03  eta: 0:00:37  time: 0.0302  data_time: 0.0113  memory: 1053  loss: 1.9845\n",
      "06/11 21:06:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 290/1563]  lr: 1.0000e-03  eta: 0:00:37  time: 0.0309  data_time: 0.0108  memory: 1053  loss: 1.9225\n",
      "06/11 21:06:51 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 300/1563]  lr: 1.0000e-03  eta: 0:00:37  time: 0.0315  data_time: 0.0112  memory: 1053  loss: 1.9206\n",
      "06/11 21:06:52 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 310/1563]  lr: 1.0000e-03  eta: 0:00:36  time: 0.0302  data_time: 0.0108  memory: 1053  loss: 1.8720\n",
      "06/11 21:06:52 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 320/1563]  lr: 1.0000e-03  eta: 0:00:36  time: 0.0306  data_time: 0.0112  memory: 1053  loss: 1.8970\n",
      "06/11 21:06:52 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 330/1563]  lr: 1.0000e-03  eta: 0:00:36  time: 0.0295  data_time: 0.0105  memory: 1053  loss: 1.9232\n",
      "06/11 21:06:52 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 340/1563]  lr: 1.0000e-03  eta: 0:00:36  time: 0.0308  data_time: 0.0120  memory: 1053  loss: 2.0134\n",
      "06/11 21:06:53 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 350/1563]  lr: 1.0000e-03  eta: 0:00:35  time: 0.0289  data_time: 0.0105  memory: 1053  loss: 1.9972\n",
      "06/11 21:06:53 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 360/1563]  lr: 1.0000e-03  eta: 0:00:35  time: 0.0272  data_time: 0.0106  memory: 1053  loss: 1.8189\n",
      "06/11 21:06:53 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 370/1563]  lr: 1.0000e-03  eta: 0:00:35  time: 0.0300  data_time: 0.0112  memory: 1053  loss: 2.0380\n",
      "06/11 21:06:54 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 380/1563]  lr: 1.0000e-03  eta: 0:00:34  time: 0.0306  data_time: 0.0116  memory: 1053  loss: 1.9283\n",
      "06/11 21:06:54 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 390/1563]  lr: 1.0000e-03  eta: 0:00:34  time: 0.0276  data_time: 0.0109  memory: 1053  loss: 2.0282\n",
      "06/11 21:06:54 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 400/1563]  lr: 1.0000e-03  eta: 0:00:34  time: 0.0291  data_time: 0.0112  memory: 1053  loss: 1.9564\n",
      "06/11 21:06:54 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 410/1563]  lr: 1.0000e-03  eta: 0:00:33  time: 0.0306  data_time: 0.0111  memory: 1053  loss: 1.8830\n",
      "06/11 21:06:55 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 420/1563]  lr: 1.0000e-03  eta: 0:00:33  time: 0.0327  data_time: 0.0112  memory: 1053  loss: 1.8268\n",
      "06/11 21:06:55 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 430/1563]  lr: 1.0000e-03  eta: 0:00:33  time: 0.0319  data_time: 0.0112  memory: 1053  loss: 1.9221\n",
      "06/11 21:06:55 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 440/1563]  lr: 1.0000e-03  eta: 0:00:33  time: 0.0310  data_time: 0.0109  memory: 1053  loss: 1.9038\n",
      "06/11 21:06:56 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 450/1563]  lr: 1.0000e-03  eta: 0:00:33  time: 0.0310  data_time: 0.0113  memory: 1053  loss: 1.8985\n",
      "06/11 21:06:56 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 460/1563]  lr: 1.0000e-03  eta: 0:00:32  time: 0.0308  data_time: 0.0117  memory: 1053  loss: 1.9061\n",
      "06/11 21:06:56 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 470/1563]  lr: 1.0000e-03  eta: 0:00:32  time: 0.0314  data_time: 0.0117  memory: 1053  loss: 1.8917\n",
      "06/11 21:06:57 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 480/1563]  lr: 1.0000e-03  eta: 0:00:32  time: 0.0293  data_time: 0.0106  memory: 1053  loss: 1.8558\n",
      "06/11 21:06:57 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 490/1563]  lr: 1.0000e-03  eta: 0:00:31  time: 0.0290  data_time: 0.0110  memory: 1053  loss: 1.8374\n",
      "06/11 21:06:57 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 500/1563]  lr: 1.0000e-03  eta: 0:00:31  time: 0.0289  data_time: 0.0112  memory: 1053  loss: 1.9419\n",
      "06/11 21:06:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 510/1563]  lr: 1.0000e-03  eta: 0:00:31  time: 0.0287  data_time: 0.0115  memory: 1053  loss: 1.9402\n",
      "06/11 21:06:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 520/1563]  lr: 1.0000e-03  eta: 0:00:30  time: 0.0290  data_time: 0.0106  memory: 1053  loss: 2.0063\n",
      "06/11 21:06:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 530/1563]  lr: 1.0000e-03  eta: 0:00:30  time: 0.0283  data_time: 0.0110  memory: 1053  loss: 1.8980\n",
      "06/11 21:06:58 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 540/1563]  lr: 1.0000e-03  eta: 0:00:30  time: 0.0277  data_time: 0.0104  memory: 1053  loss: 1.9936\n",
      "06/11 21:06:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 550/1563]  lr: 1.0000e-03  eta: 0:00:29  time: 0.0245  data_time: 0.0096  memory: 1053  loss: 1.9660\n",
      "06/11 21:06:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 560/1563]  lr: 1.0000e-03  eta: 0:00:29  time: 0.0296  data_time: 0.0108  memory: 1053  loss: 1.9638\n",
      "06/11 21:06:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 570/1563]  lr: 1.0000e-03  eta: 0:00:29  time: 0.0289  data_time: 0.0108  memory: 1053  loss: 2.0083\n",
      "06/11 21:06:59 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 580/1563]  lr: 1.0000e-03  eta: 0:00:28  time: 0.0273  data_time: 0.0091  memory: 1053  loss: 1.9306\n",
      "06/11 21:07:00 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 590/1563]  lr: 1.0000e-03  eta: 0:00:28  time: 0.0304  data_time: 0.0114  memory: 1053  loss: 1.9372\n",
      "06/11 21:07:00 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 600/1563]  lr: 1.0000e-03  eta: 0:00:28  time: 0.0290  data_time: 0.0107  memory: 1053  loss: 1.9776\n",
      "06/11 21:07:00 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 610/1563]  lr: 1.0000e-03  eta: 0:00:28  time: 0.0284  data_time: 0.0115  memory: 1053  loss: 1.8172\n",
      "06/11 21:07:01 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 620/1563]  lr: 1.0000e-03  eta: 0:00:27  time: 0.0270  data_time: 0.0103  memory: 1053  loss: 1.8375\n",
      "06/11 21:07:01 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 630/1563]  lr: 1.0000e-03  eta: 0:00:27  time: 0.0297  data_time: 0.0109  memory: 1053  loss: 1.8611\n",
      "06/11 21:07:01 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 640/1563]  lr: 1.0000e-03  eta: 0:00:27  time: 0.0290  data_time: 0.0109  memory: 1053  loss: 1.8062\n",
      "06/11 21:07:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 650/1563]  lr: 1.0000e-03  eta: 0:00:26  time: 0.0299  data_time: 0.0106  memory: 1053  loss: 1.8104\n",
      "06/11 21:07:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 660/1563]  lr: 1.0000e-03  eta: 0:00:26  time: 0.0282  data_time: 0.0113  memory: 1053  loss: 1.9975\n",
      "06/11 21:07:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 670/1563]  lr: 1.0000e-03  eta: 0:00:26  time: 0.0293  data_time: 0.0111  memory: 1053  loss: 1.9203\n",
      "06/11 21:07:02 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 680/1563]  lr: 1.0000e-03  eta: 0:00:25  time: 0.0292  data_time: 0.0106  memory: 1053  loss: 1.8921\n",
      "06/11 21:07:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 690/1563]  lr: 1.0000e-03  eta: 0:00:25  time: 0.0285  data_time: 0.0107  memory: 1053  loss: 1.9201\n",
      "06/11 21:07:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 700/1563]  lr: 1.0000e-03  eta: 0:00:25  time: 0.0308  data_time: 0.0114  memory: 1053  loss: 1.8728\n",
      "06/11 21:07:03 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 710/1563]  lr: 1.0000e-03  eta: 0:00:25  time: 0.0312  data_time: 0.0117  memory: 1053  loss: 1.8248\n",
      "06/11 21:07:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 720/1563]  lr: 1.0000e-03  eta: 0:00:24  time: 0.0298  data_time: 0.0112  memory: 1053  loss: 1.9042\n",
      "06/11 21:07:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 730/1563]  lr: 1.0000e-03  eta: 0:00:24  time: 0.0291  data_time: 0.0109  memory: 1053  loss: 1.9433\n",
      "06/11 21:07:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 740/1563]  lr: 1.0000e-03  eta: 0:00:24  time: 0.0292  data_time: 0.0112  memory: 1053  loss: 1.9202\n",
      "06/11 21:07:04 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 750/1563]  lr: 1.0000e-03  eta: 0:00:23  time: 0.0287  data_time: 0.0111  memory: 1053  loss: 1.8865\n",
      "06/11 21:07:05 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 760/1563]  lr: 1.0000e-03  eta: 0:00:23  time: 0.0303  data_time: 0.0108  memory: 1053  loss: 1.7672\n",
      "06/11 21:07:05 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 770/1563]  lr: 1.0000e-03  eta: 0:00:23  time: 0.0303  data_time: 0.0111  memory: 1053  loss: 1.9461\n",
      "06/11 21:07:05 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 780/1563]  lr: 1.0000e-03  eta: 0:00:23  time: 0.0293  data_time: 0.0109  memory: 1053  loss: 1.8363\n",
      "06/11 21:07:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 790/1563]  lr: 1.0000e-03  eta: 0:00:22  time: 0.0299  data_time: 0.0115  memory: 1053  loss: 1.9302\n",
      "06/11 21:07:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 800/1563]  lr: 1.0000e-03  eta: 0:00:22  time: 0.0297  data_time: 0.0108  memory: 1053  loss: 1.7968\n",
      "06/11 21:07:06 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 810/1563]  lr: 1.0000e-03  eta: 0:00:22  time: 0.0300  data_time: 0.0117  memory: 1053  loss: 1.8891\n",
      "06/11 21:07:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 820/1563]  lr: 1.0000e-03  eta: 0:00:21  time: 0.0309  data_time: 0.0110  memory: 1053  loss: 1.8506\n",
      "06/11 21:07:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 830/1563]  lr: 1.0000e-03  eta: 0:00:21  time: 0.0313  data_time: 0.0111  memory: 1053  loss: 1.9017\n",
      "06/11 21:07:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 840/1563]  lr: 1.0000e-03  eta: 0:00:21  time: 0.0312  data_time: 0.0112  memory: 1053  loss: 1.8362\n",
      "06/11 21:07:07 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 850/1563]  lr: 1.0000e-03  eta: 0:00:21  time: 0.0309  data_time: 0.0113  memory: 1053  loss: 1.9188\n",
      "06/11 21:07:08 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 860/1563]  lr: 1.0000e-03  eta: 0:00:20  time: 0.0308  data_time: 0.0111  memory: 1053  loss: 1.8842\n",
      "06/11 21:07:08 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 870/1563]  lr: 1.0000e-03  eta: 0:00:20  time: 0.0311  data_time: 0.0117  memory: 1053  loss: 1.9008\n",
      "06/11 21:07:08 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 880/1563]  lr: 1.0000e-03  eta: 0:00:20  time: 0.0278  data_time: 0.0108  memory: 1053  loss: 1.8404\n",
      "06/11 21:07:09 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 890/1563]  lr: 1.0000e-03  eta: 0:00:19  time: 0.0279  data_time: 0.0105  memory: 1053  loss: 1.8383\n",
      "06/11 21:07:09 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 900/1563]  lr: 1.0000e-03  eta: 0:00:19  time: 0.0298  data_time: 0.0117  memory: 1053  loss: 1.8134\n",
      "06/11 21:07:09 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 910/1563]  lr: 1.0000e-03  eta: 0:00:19  time: 0.0262  data_time: 0.0101  memory: 1053  loss: 1.9590\n",
      "06/11 21:07:09 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 920/1563]  lr: 1.0000e-03  eta: 0:00:18  time: 0.0258  data_time: 0.0100  memory: 1053  loss: 1.9051\n",
      "06/11 21:07:10 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 930/1563]  lr: 1.0000e-03  eta: 0:00:18  time: 0.0291  data_time: 0.0109  memory: 1053  loss: 1.8474\n",
      "06/11 21:07:10 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 940/1563]  lr: 1.0000e-03  eta: 0:00:18  time: 0.0302  data_time: 0.0118  memory: 1053  loss: 1.8492\n",
      "06/11 21:07:10 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 950/1563]  lr: 1.0000e-03  eta: 0:00:18  time: 0.0302  data_time: 0.0112  memory: 1053  loss: 1.8054\n",
      "06/11 21:07:11 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 960/1563]  lr: 1.0000e-03  eta: 0:00:17  time: 0.0296  data_time: 0.0117  memory: 1053  loss: 1.9229\n",
      "06/11 21:07:11 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 970/1563]  lr: 1.0000e-03  eta: 0:00:17  time: 0.0289  data_time: 0.0107  memory: 1053  loss: 1.8993\n",
      "06/11 21:07:11 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 980/1563]  lr: 1.0000e-03  eta: 0:00:17  time: 0.0294  data_time: 0.0115  memory: 1053  loss: 1.8540\n",
      "06/11 21:07:12 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][ 990/1563]  lr: 1.0000e-03  eta: 0:00:16  time: 0.0307  data_time: 0.0112  memory: 1053  loss: 1.9288\n",
      "06/11 21:07:12 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: 20230611_210642\n",
      "06/11 21:07:12 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1000/1563]  lr: 1.0000e-03  eta: 0:00:16  time: 0.0280  data_time: 0.0107  memory: 1053  loss: 1.8614\n",
      "06/11 21:07:12 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1010/1563]  lr: 1.0000e-03  eta: 0:00:16  time: 0.0283  data_time: 0.0110  memory: 1053  loss: 1.9108\n",
      "06/11 21:07:12 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1020/1563]  lr: 1.0000e-03  eta: 0:00:15  time: 0.0294  data_time: 0.0112  memory: 1053  loss: 1.8226\n",
      "06/11 21:07:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1030/1563]  lr: 1.0000e-03  eta: 0:00:15  time: 0.0307  data_time: 0.0117  memory: 1053  loss: 1.9285\n",
      "06/11 21:07:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1040/1563]  lr: 1.0000e-03  eta: 0:00:15  time: 0.0303  data_time: 0.0115  memory: 1053  loss: 1.8204\n",
      "06/11 21:07:13 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1050/1563]  lr: 1.0000e-03  eta: 0:00:15  time: 0.0283  data_time: 0.0106  memory: 1053  loss: 1.7534\n",
      "06/11 21:07:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1060/1563]  lr: 1.0000e-03  eta: 0:00:14  time: 0.0292  data_time: 0.0109  memory: 1053  loss: 1.9953\n",
      "06/11 21:07:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1070/1563]  lr: 1.0000e-03  eta: 0:00:14  time: 0.0291  data_time: 0.0112  memory: 1053  loss: 1.7784\n",
      "06/11 21:07:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1080/1563]  lr: 1.0000e-03  eta: 0:00:14  time: 0.0267  data_time: 0.0102  memory: 1053  loss: 1.8513\n",
      "06/11 21:07:14 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1090/1563]  lr: 1.0000e-03  eta: 0:00:13  time: 0.0292  data_time: 0.0107  memory: 1053  loss: 1.8147\n",
      "06/11 21:07:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1100/1563]  lr: 1.0000e-03  eta: 0:00:13  time: 0.0301  data_time: 0.0106  memory: 1053  loss: 1.9660\n",
      "06/11 21:07:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1110/1563]  lr: 1.0000e-03  eta: 0:00:13  time: 0.0298  data_time: 0.0115  memory: 1053  loss: 1.9000\n",
      "06/11 21:07:15 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1120/1563]  lr: 1.0000e-03  eta: 0:00:13  time: 0.0297  data_time: 0.0108  memory: 1053  loss: 1.8412\n",
      "06/11 21:07:16 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1130/1563]  lr: 1.0000e-03  eta: 0:00:12  time: 0.0294  data_time: 0.0105  memory: 1053  loss: 1.8553\n",
      "06/11 21:07:16 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1140/1563]  lr: 1.0000e-03  eta: 0:00:12  time: 0.0289  data_time: 0.0109  memory: 1053  loss: 1.8482\n",
      "06/11 21:07:16 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1150/1563]  lr: 1.0000e-03  eta: 0:00:12  time: 0.0298  data_time: 0.0114  memory: 1053  loss: 1.8325\n",
      "06/11 21:07:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1160/1563]  lr: 1.0000e-03  eta: 0:00:11  time: 0.0282  data_time: 0.0107  memory: 1053  loss: 1.9020\n",
      "06/11 21:07:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1170/1563]  lr: 1.0000e-03  eta: 0:00:11  time: 0.0273  data_time: 0.0112  memory: 1053  loss: 1.8687\n",
      "06/11 21:07:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1180/1563]  lr: 1.0000e-03  eta: 0:00:11  time: 0.0305  data_time: 0.0118  memory: 1053  loss: 1.8894\n",
      "06/11 21:07:17 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1190/1563]  lr: 1.0000e-03  eta: 0:00:10  time: 0.0276  data_time: 0.0108  memory: 1053  loss: 1.8849\n",
      "06/11 21:07:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1200/1563]  lr: 1.0000e-03  eta: 0:00:10  time: 0.0278  data_time: 0.0110  memory: 1053  loss: 1.9108\n",
      "06/11 21:07:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1210/1563]  lr: 1.0000e-03  eta: 0:00:10  time: 0.0290  data_time: 0.0113  memory: 1053  loss: 1.8176\n",
      "06/11 21:07:18 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1220/1563]  lr: 1.0000e-03  eta: 0:00:10  time: 0.0277  data_time: 0.0102  memory: 1053  loss: 1.8871\n",
      "06/11 21:07:19 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1230/1563]  lr: 1.0000e-03  eta: 0:00:09  time: 0.0275  data_time: 0.0109  memory: 1053  loss: 1.9396\n",
      "06/11 21:07:19 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1240/1563]  lr: 1.0000e-03  eta: 0:00:09  time: 0.0274  data_time: 0.0106  memory: 1053  loss: 1.7774\n",
      "06/11 21:07:19 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1250/1563]  lr: 1.0000e-03  eta: 0:00:09  time: 0.0282  data_time: 0.0103  memory: 1053  loss: 1.7857\n",
      "06/11 21:07:19 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1260/1563]  lr: 1.0000e-03  eta: 0:00:08  time: 0.0278  data_time: 0.0107  memory: 1053  loss: 1.8487\n",
      "06/11 21:07:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1270/1563]  lr: 1.0000e-03  eta: 0:00:08  time: 0.0260  data_time: 0.0104  memory: 1053  loss: 1.9328\n",
      "06/11 21:07:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1280/1563]  lr: 1.0000e-03  eta: 0:00:08  time: 0.0269  data_time: 0.0106  memory: 1053  loss: 1.9666\n",
      "06/11 21:07:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1290/1563]  lr: 1.0000e-03  eta: 0:00:07  time: 0.0308  data_time: 0.0112  memory: 1053  loss: 1.8695\n",
      "06/11 21:07:20 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1300/1563]  lr: 1.0000e-03  eta: 0:00:07  time: 0.0296  data_time: 0.0107  memory: 1053  loss: 1.8347\n",
      "06/11 21:07:21 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1310/1563]  lr: 1.0000e-03  eta: 0:00:07  time: 0.0310  data_time: 0.0111  memory: 1053  loss: 1.8468\n",
      "06/11 21:07:21 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1320/1563]  lr: 1.0000e-03  eta: 0:00:07  time: 0.0316  data_time: 0.0113  memory: 1053  loss: 1.8653\n",
      "06/11 21:07:21 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1330/1563]  lr: 1.0000e-03  eta: 0:00:06  time: 0.0298  data_time: 0.0104  memory: 1053  loss: 1.9002\n",
      "06/11 21:07:22 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1340/1563]  lr: 1.0000e-03  eta: 0:00:06  time: 0.0311  data_time: 0.0112  memory: 1053  loss: 1.8688\n",
      "06/11 21:07:22 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1350/1563]  lr: 1.0000e-03  eta: 0:00:06  time: 0.0316  data_time: 0.0117  memory: 1053  loss: 1.8055\n",
      "06/11 21:07:22 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1360/1563]  lr: 1.0000e-03  eta: 0:00:05  time: 0.0299  data_time: 0.0111  memory: 1053  loss: 1.8030\n",
      "06/11 21:07:23 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1370/1563]  lr: 1.0000e-03  eta: 0:00:05  time: 0.0313  data_time: 0.0109  memory: 1053  loss: 1.8885\n",
      "06/11 21:07:23 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1380/1563]  lr: 1.0000e-03  eta: 0:00:05  time: 0.0307  data_time: 0.0110  memory: 1053  loss: 1.8504\n",
      "06/11 21:07:23 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1390/1563]  lr: 1.0000e-03  eta: 0:00:05  time: 0.0301  data_time: 0.0107  memory: 1053  loss: 1.7713\n",
      "06/11 21:07:24 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1400/1563]  lr: 1.0000e-03  eta: 0:00:04  time: 0.0295  data_time: 0.0111  memory: 1053  loss: 1.9067\n",
      "06/11 21:07:24 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1410/1563]  lr: 1.0000e-03  eta: 0:00:04  time: 0.0308  data_time: 0.0116  memory: 1053  loss: 1.8280\n",
      "06/11 21:07:24 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1420/1563]  lr: 1.0000e-03  eta: 0:00:04  time: 0.0310  data_time: 0.0105  memory: 1053  loss: 1.7944\n",
      "06/11 21:07:24 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1430/1563]  lr: 1.0000e-03  eta: 0:00:03  time: 0.0315  data_time: 0.0107  memory: 1053  loss: 1.9695\n",
      "06/11 21:07:25 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1440/1563]  lr: 1.0000e-03  eta: 0:00:03  time: 0.0315  data_time: 0.0116  memory: 1053  loss: 1.7941\n",
      "06/11 21:07:25 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1450/1563]  lr: 1.0000e-03  eta: 0:00:03  time: 0.0314  data_time: 0.0119  memory: 1053  loss: 1.8492\n",
      "06/11 21:07:25 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1460/1563]  lr: 1.0000e-03  eta: 0:00:03  time: 0.0299  data_time: 0.0109  memory: 1053  loss: 1.8184\n",
      "06/11 21:07:26 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1470/1563]  lr: 1.0000e-03  eta: 0:00:02  time: 0.0301  data_time: 0.0115  memory: 1053  loss: 1.8929\n",
      "06/11 21:07:26 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1480/1563]  lr: 1.0000e-03  eta: 0:00:02  time: 0.0292  data_time: 0.0115  memory: 1053  loss: 1.8611\n",
      "06/11 21:07:26 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1490/1563]  lr: 1.0000e-03  eta: 0:00:02  time: 0.0296  data_time: 0.0113  memory: 1053  loss: 2.0090\n",
      "06/11 21:07:27 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1500/1563]  lr: 1.0000e-03  eta: 0:00:01  time: 0.0286  data_time: 0.0115  memory: 1053  loss: 1.8075\n",
      "06/11 21:07:27 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1510/1563]  lr: 1.0000e-03  eta: 0:00:01  time: 0.0288  data_time: 0.0104  memory: 1053  loss: 1.9249\n",
      "06/11 21:07:27 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1520/1563]  lr: 1.0000e-03  eta: 0:00:01  time: 0.0311  data_time: 0.0118  memory: 1053  loss: 1.7560\n",
      "06/11 21:07:27 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1530/1563]  lr: 1.0000e-03  eta: 0:00:00  time: 0.0306  data_time: 0.0113  memory: 1053  loss: 1.7856\n",
      "06/11 21:07:28 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1540/1563]  lr: 1.0000e-03  eta: 0:00:00  time: 0.0284  data_time: 0.0109  memory: 1053  loss: 1.8102\n",
      "06/11 21:07:28 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1550/1563]  lr: 1.0000e-03  eta: 0:00:00  time: 0.0268  data_time: 0.0106  memory: 1053  loss: 1.8003\n",
      "06/11 21:07:28 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(train) [1][1560/1563]  lr: 1.0000e-03  eta: 0:00:00  time: 0.0313  data_time: 0.0117  memory: 1053  loss: 1.9632\n",
      "06/11 21:07:28 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Exp name: 20230611_210642\n",
      "06/11 21:07:28 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Saving checkpoint at 1 epochs\n",
      "06/11 21:07:29 - mmengine - \u001b[5m\u001b[4m\u001b[33mWARNING\u001b[0m - `save_param_scheduler` is True but `self.param_schedulers` is None, so skip saving parameter schedulers\n",
      "06/11 21:07:29 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][ 10/313]    eta: 0:00:02  time: 0.0096  data_time: 0.0049  memory: 1053  \n",
      "06/11 21:07:29 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][ 20/313]    eta: 0:00:02  time: 0.0098  data_time: 0.0049  memory: 909  \n",
      "06/11 21:07:29 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][ 30/313]    eta: 0:00:02  time: 0.0099  data_time: 0.0050  memory: 909  \n",
      "06/11 21:07:29 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][ 40/313]    eta: 0:00:02  time: 0.0101  data_time: 0.0054  memory: 909  \n",
      "06/11 21:07:29 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][ 50/313]    eta: 0:00:02  time: 0.0096  data_time: 0.0050  memory: 909  \n",
      "06/11 21:07:29 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][ 60/313]    eta: 0:00:02  time: 0.0095  data_time: 0.0049  memory: 909  \n",
      "06/11 21:07:29 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][ 70/313]    eta: 0:00:02  time: 0.0099  data_time: 0.0051  memory: 909  \n",
      "06/11 21:07:30 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][ 80/313]    eta: 0:00:02  time: 0.0094  data_time: 0.0049  memory: 909  \n",
      "06/11 21:07:30 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][ 90/313]    eta: 0:00:02  time: 0.0100  data_time: 0.0051  memory: 909  \n",
      "06/11 21:07:30 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][100/313]    eta: 0:00:02  time: 0.0107  data_time: 0.0056  memory: 909  \n",
      "06/11 21:07:30 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][110/313]    eta: 0:00:01  time: 0.0095  data_time: 0.0049  memory: 909  \n",
      "06/11 21:07:30 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][120/313]    eta: 0:00:01  time: 0.0096  data_time: 0.0050  memory: 909  \n",
      "06/11 21:07:30 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][130/313]    eta: 0:00:01  time: 0.0101  data_time: 0.0052  memory: 909  \n",
      "06/11 21:07:30 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][140/313]    eta: 0:00:01  time: 0.0098  data_time: 0.0050  memory: 909  \n",
      "06/11 21:07:30 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][150/313]    eta: 0:00:01  time: 0.0098  data_time: 0.0050  memory: 909  \n",
      "06/11 21:07:30 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][160/313]    eta: 0:00:01  time: 0.0102  data_time: 0.0053  memory: 909  \n",
      "06/11 21:07:30 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][170/313]    eta: 0:00:01  time: 0.0098  data_time: 0.0050  memory: 909  \n",
      "06/11 21:07:31 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][180/313]    eta: 0:00:01  time: 0.0105  data_time: 0.0052  memory: 909  \n",
      "06/11 21:07:31 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][190/313]    eta: 0:00:01  time: 0.0100  data_time: 0.0051  memory: 909  \n",
      "06/11 21:07:31 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][200/313]    eta: 0:00:01  time: 0.0099  data_time: 0.0050  memory: 909  \n",
      "06/11 21:07:31 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][210/313]    eta: 0:00:01  time: 0.0097  data_time: 0.0050  memory: 909  \n",
      "06/11 21:07:31 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][220/313]    eta: 0:00:00  time: 0.0097  data_time: 0.0050  memory: 909  \n",
      "06/11 21:07:31 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][230/313]    eta: 0:00:00  time: 0.0097  data_time: 0.0050  memory: 909  \n",
      "06/11 21:07:31 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][240/313]    eta: 0:00:00  time: 0.0099  data_time: 0.0050  memory: 909  \n",
      "06/11 21:07:31 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][250/313]    eta: 0:00:00  time: 0.0100  data_time: 0.0051  memory: 909  \n",
      "06/11 21:07:31 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][260/313]    eta: 0:00:00  time: 0.0099  data_time: 0.0050  memory: 909  \n",
      "06/11 21:07:31 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][270/313]    eta: 0:00:00  time: 0.0097  data_time: 0.0050  memory: 909  \n",
      "06/11 21:07:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][280/313]    eta: 0:00:00  time: 0.0099  data_time: 0.0051  memory: 909  \n",
      "06/11 21:07:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][290/313]    eta: 0:00:00  time: 0.0099  data_time: 0.0051  memory: 909  \n",
      "06/11 21:07:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][300/313]    eta: 0:00:00  time: 0.0101  data_time: 0.0051  memory: 909  \n",
      "06/11 21:07:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][310/313]    eta: 0:00:00  time: 0.0100  data_time: 0.0051  memory: 909  \n",
      "06/11 21:07:32 - mmengine - \u001b[4m\u001b[97mINFO\u001b[0m - Epoch(val) [1][313/313]    accuracy: 37.6100  data_time: 0.0051  time: 0.0099\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MMConvNeXtTiny(\n",
       "  (data_preprocessor): BaseDataPreprocessor()\n",
       "  (model): ConvNeXt(\n",
       "    (features): Sequential(\n",
       "      (0): Conv2dNormActivation(\n",
       "        (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "        (1): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "        )\n",
       "        (1): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0058823529411764705, mode=row)\n",
       "        )\n",
       "        (2): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=384, out_features=96, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.011764705882352941, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): LayerNorm2d((96,), eps=1e-06, elementwise_affine=True)\n",
       "        (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.017647058823529415, mode=row)\n",
       "        )\n",
       "        (1): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.023529411764705882, mode=row)\n",
       "        )\n",
       "        (2): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=768, out_features=192, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.029411764705882353, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): LayerNorm2d((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))\n",
       "      )\n",
       "      (5): Sequential(\n",
       "        (0): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.03529411764705883, mode=row)\n",
       "        )\n",
       "        (1): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0411764705882353, mode=row)\n",
       "        )\n",
       "        (2): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.047058823529411764, mode=row)\n",
       "        )\n",
       "        (3): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.052941176470588235, mode=row)\n",
       "        )\n",
       "        (4): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.058823529411764705, mode=row)\n",
       "        )\n",
       "        (5): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.06470588235294118, mode=row)\n",
       "        )\n",
       "        (6): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.07058823529411766, mode=row)\n",
       "        )\n",
       "        (7): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.07647058823529412, mode=row)\n",
       "        )\n",
       "        (8): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=1536, out_features=384, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.0823529411764706, mode=row)\n",
       "        )\n",
       "      )\n",
       "      (6): Sequential(\n",
       "        (0): LayerNorm2d((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))\n",
       "      )\n",
       "      (7): Sequential(\n",
       "        (0): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.08823529411764706, mode=row)\n",
       "        )\n",
       "        (1): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.09411764705882353, mode=row)\n",
       "        )\n",
       "        (2): CNBlock(\n",
       "          (block): Sequential(\n",
       "            (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)\n",
       "            (1): Permute()\n",
       "            (2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "            (3): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (4): GELU(approximate='none')\n",
       "            (5): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (6): Permute()\n",
       "          )\n",
       "          (stochastic_depth): StochasticDepth(p=0.1, mode=row)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "    (classifier): Sequential(\n",
       "      (0): LayerNorm2d((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (1): Flatten(start_dim=1, end_dim=-1)\n",
       "      (2): Linear(in_features=768, out_features=1000, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TRAIN THIS PUPPY!\n",
    "runner.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3112",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
